{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQJY3d07U31q",
        "outputId": "08876f24-3e0f-4823-f4f8-04a097a92d5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers sentencepiece torch torchvision albumentations timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import timm"
      ],
      "metadata": {
        "id": "daXvC2xbXHUR"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHz0psxzamY2",
        "outputId": "cecf0350-fd22-40fb-880a-87c1214cbee8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "EclmxmhBambV"
      },
      "outputs": [],
      "source": [
        "# Define paths to the image folder and caption file\n",
        "image_folder = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "caption_file = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define configuration with adjusted batch size\n",
        "class CFG:\n",
        "    model_name = \"resnet50\"\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"  # Bangla BERT model\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    batch_size = 16  # Reduced batch size\n",
        "    size = 224\n",
        "    image_embedding = 2048\n",
        "    text_embedding = 768\n",
        "    projection_dim = 512\n",
        "    max_length = 128\n",
        "    temperature = 0.07\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    head_lr = 1e-6\n",
        "    weight_decay = 1e-4\n",
        "    patience = 2\n",
        "    factor = 0.5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    epochs = 15\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "egxSdt-RRfo6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Dataset class\n",
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "        image = cv2.imread(f\"{image_folder}/{self.image_filenames[idx]}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)['image']\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['caption'] = self.captions[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)"
      ],
      "metadata": {
        "id": "v2710991p8q9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Image Encoder\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "dKQ0xymRVvaH"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "7iyNR7Gmammo"
      },
      "outputs": [],
      "source": [
        "# Text Encoder\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "oWR3sEhSampP"
      },
      "outputs": [],
      "source": [
        "# Projection Head\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, projection_dim=CFG.projection_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# CLIP Model\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self, temperature=CFG.temperature):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=CFG.image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=CFG.text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax((images_similarity + texts_similarity) / 2 * self.temperature, dim=-1)\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "2159Cm_0T370"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "7bcD0-znamwl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cross Entropy\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    return loss.mean() if reduction == \"mean\" else loss\n",
        "\n",
        "# Data loaders\n",
        "def get_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K8djbMmdJOx",
        "outputId": "bd65168e-49bc-42f9-8c18-0eb90828e812"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'caption_id': '1.jpg', 'bengali_caption': '  গ্রামে হাঁটা দুই শিশু।'}\n"
          ]
        }
      ],
      "source": [
        "with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Print a sample item to inspect the structure\n",
        "print(captions_data[0])  # Check the first item for key names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "J7eEnEjqeq86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F  # Add this line for functional operations\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/Bangla Image dataset with caption/final_clip_model.zip'\n",
        "extract_path = '/content/drive/MyDrive/Bangla Image dataset with caption/'\n",
        "\n",
        "# Unzip the model\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Model unzipped successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byNhhLfTCJvf",
        "outputId": "03288493-bf94-4109-f5ba-5d86d201b33f"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model unzipped successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import os  # For checking and creating directories if needed\n",
        "\n",
        "# Optimized train_epoch with mixed precision\n",
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Mixed precision scaler\n",
        "    loss_meter = 0\n",
        "    for batch in tqdm(train_loader, total=len(train_loader)):\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision training\n",
        "            loss = model(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        loss_meter += loss.item()\n",
        "    return loss_meter / len(train_loader)\n",
        "\n",
        "# Validation function remains the same\n",
        "def valid_epoch(model, valid_loader):\n",
        "    model.eval()\n",
        "    loss_meter = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, total=len(valid_loader)):\n",
        "            batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "            loss = model(batch)\n",
        "            loss_meter += loss.item()\n",
        "    return loss_meter / len(valid_loader)\n",
        "\n",
        "# Main function with optimizations for data loading and mixed precision training\n",
        "def main():\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "        captions_data = json.load(f)\n",
        "\n",
        "    # Extract image files and captions based on provided structure\n",
        "    image_files = [item['caption_id'].split('#')[0] for item in captions_data]\n",
        "    captions = [item['bengali_caption'] for item in captions_data]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "    transforms = get_transforms()\n",
        "    dataset = CLIPDataset(image_files, captions, tokenizer, transforms)\n",
        "\n",
        "    # Data splitting into train and validation sets\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "    # DataLoader with num_workers for faster data loading\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "\n",
        "    # Load pre-trained weights if they exist\n",
        "    pre_trained_model_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/final_clip_model.pt\"\n",
        "    if os.path.exists(pre_trained_model_path):\n",
        "        model.load_state_dict(torch.load(pre_trained_model_path, map_location=CFG.device))\n",
        "        print(f\"Loaded pre-trained model from {pre_trained_model_path}\")\n",
        "    else:\n",
        "        print(f\"No pre-trained model found at {pre_trained_model_path}. Starting training from scratch.\")\n",
        "\n",
        "    model.train()  # Set model to training mode if fine-tuning\n",
        "\n",
        "    # Define optimizer and learning rate scheduler\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()),\n",
        "         \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.0)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor)\n",
        "    step = \"epoch\"\n",
        "\n",
        "    # Variable to track best validation loss\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    # Start training loop\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{CFG.epochs}\")\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        train_losses.append(train_loss)  # Store the train loss\n",
        "\n",
        "        valid_loss = valid_epoch(model, valid_loader)\n",
        "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
        "        valid_losses.append(valid_loss)  # Store the validation loss\n",
        "\n",
        "        if step == \"epoch\":\n",
        "            lr_scheduler.step(valid_loss)\n",
        "\n",
        "        # Save the model after each epoch in Google Drive\n",
        "        model_save_path = f\"/content/drive/MyDrive/Bangla Image dataset with caption/epoch_{epoch+1}_clip_model.pt\"\n",
        "        torch.save(model.state_dict(), model_save_path)\n",
        "        print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "        # Save the model if it achieves the best validation loss so far\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            print(f\"Validation loss decreased. Saving the best model...\")\n",
        "            torch.save(model.state_dict(), \"/content/drive/MyDrive/Bangla Image dataset with caption/best_clip_model.pt\")\n",
        "\n",
        "    # Plotting the train and validation loss after the training loop\n",
        "    plot_losses(train_losses, valid_losses)\n",
        "\n",
        "    # Optionally save the final model\n",
        "    final_model_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/final_clip_model.pt\"\n",
        "    torch.save(model.state_dict(), final_model_path)  # Save the final model after training\n",
        "\n",
        "def plot_losses(train_losses, valid_losses):\n",
        "    \"\"\"Plot train and validation losses\"\"\"\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue', linestyle='-', marker='o')\n",
        "    plt.plot(valid_losses, label='Validation Loss', color='red', linestyle='-', marker='x')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Run the training without early stopping\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "id": "1_blLKxAZHJx",
        "outputId": "bb8ad6b8-e397-4ace-bee7-cb2967581d1e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-75-a46b4766ce3c>:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(pre_trained_model_path, map_location=CFG.device))\n",
            "<ipython-input-75-a46b4766ce3c>:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # Mixed precision scaler\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pre-trained model from /content/drive/MyDrive/Bangla Image dataset with caption/final_clip_model.pt\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/1956 [00:00<?, ?it/s]<ipython-input-75-a46b4766ce3c>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision training\n",
            "100%|██████████| 1956/1956 [05:24<00:00,  6.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 489/489 [00:45<00:00, 10.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.4020\n",
            "Model saved to /content/drive/MyDrive/Bangla Image dataset with caption/epoch_1_clip_model.pt\n",
            "Validation loss decreased. Saving the best model...\n",
            "Epoch 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1956/1956 [05:17<00:00,  6.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 489/489 [00:44<00:00, 10.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.3914\n",
            "Model saved to /content/drive/MyDrive/Bangla Image dataset with caption/epoch_2_clip_model.pt\n",
            "Validation loss decreased. Saving the best model...\n",
            "Epoch 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1956/1956 [05:22<00:00,  6.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 489/489 [00:45<00:00, 10.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.3981\n",
            "Model saved to /content/drive/MyDrive/Bangla Image dataset with caption/epoch_3_clip_model.pt\n",
            "Epoch 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1956/1956 [05:21<00:00,  6.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3944\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 275/489 [00:25<00:19, 10.76it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-a46b4766ce3c>\u001b[0m in \u001b[0;36m<cell line: 142>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;31m# Run the training without early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-75-a46b4766ce3c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Store the train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation Loss: {valid_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mvalid_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Store the validation loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-a46b4766ce3c>\u001b[0m in \u001b[0;36mvalid_epoch\u001b[0;34m(model, valid_loader)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"caption\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss_meter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_meter\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#iNTERFACE"
      ],
      "metadata": {
        "id": "E15lcPJehFjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Assuming CLIPDataset and CFG have already been defined\n",
        "# Ensure the paths to the dataset are properly set\n",
        "\n",
        "# Load and prepare the dataset\n",
        "with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Prepare image files and captions\n",
        "image_files = [item['caption_id'].split('#')[0] for item in captions_data]\n",
        "captions = [item['bengali_caption'] for item in captions_data]\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "transforms = get_transforms()\n",
        "\n",
        "# Create the dataset\n",
        "dataset = CLIPDataset(image_files, captions, tokenizer, transforms)\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "_, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "# Define the function to get image embeddings\n",
        "def get_image_embeddings(valid_dataset, model_path):\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))  # Load final model\n",
        "    model.eval()\n",
        "\n",
        "    valid_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            # Move image tensors to the correct device\n",
        "            images = batch[\"image\"].to(CFG.device)\n",
        "\n",
        "            # Get image features from the encoder\n",
        "            image_features = model.image_encoder(images)\n",
        "\n",
        "            # Project image features into the desired embedding space\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "\n",
        "            # Append the image embeddings to the list\n",
        "            valid_image_embeddings.append(image_embeddings)\n",
        "\n",
        "    # Concatenate all image embeddings into a single tensor and return\n",
        "    return model, torch.cat(valid_image_embeddings)\n",
        "\n",
        "# Perform inference to get image embeddings from the validation set\n",
        "model, image_embeddings = get_image_embeddings(valid_dataset, \"/content/final_clip_model.pt\")  # Use final_clip_model.pt\n",
        "\n",
        "# Example usage of the image embeddings\n",
        "print(f\"Image Embeddings shape: {image_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "VTERU4Nt_Dxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  # Add this import\n"
      ],
      "metadata": {
        "id": "Dd7-DzquNn2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def find_matches(model, image_embeddings, query, image_files, n=1):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "\n",
        "    # Encode the query\n",
        "    encoded_query = tokenizer([query], return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.max_length)\n",
        "    print(f\"Encoded Query: {encoded_query}\")  # For debugging\n",
        "\n",
        "    # Prepare the batch for the model\n",
        "    batch = {\n",
        "        key: torch.tensor(values).to(CFG.device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get text features from the model\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        # Project text features into the same space as image features\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "    # Normalize the embeddings\n",
        "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "\n",
        "    # Calculate cosine similarity between the query and all image embeddings\n",
        "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "    values, indices = torch.topk(dot_similarity.squeeze(0), n)\n",
        "\n",
        "    # Get the top match (most similar image)\n",
        "    top_match = image_files[indices[0]]  # The most similar image file\n",
        "    top_similarity_score = values[0].item()  # The similarity score for the top match\n",
        "\n",
        "    # Output the results\n",
        "    print(f\"Top match: {top_match} with similarity score: {top_similarity_score:.4f}\")\n",
        "\n",
        "    # Read and display the top matching image\n",
        "    image = cv2.imread(f\"{image_folder}/{top_match}\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Plot the image with the similarity score in the title\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis(\"off\")  # Hide axes\n",
        "    plt.title(f\"Similarity Score: {top_similarity_score:.4f}\")  # Display similarity score\n",
        "    plt.show()\n",
        "\n",
        "# Example usage with the query prompt\n",
        "prompt = \"কুকুর\"  # Example prompt for \"A dog\"\n",
        "find_matches(model, image_embeddings, prompt, image_files)\n"
      ],
      "metadata": {
        "id": "zZUyL4FCYq5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Update model path to the final model\n",
        "model_path = '/content/final_clip_model.pt'\n",
        "zip_path = '/content/final_clip_model.zip'\n",
        "\n",
        "# Creating a zip file\n",
        "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    zipf.write(model_path, os.path.basename(model_path))\n",
        "\n",
        "print(f\"Model has been zipped and saved as {zip_path}\")\n"
      ],
      "metadata": {
        "id": "EenpEbfw6JW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Trigger file download\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "jgPinh-u0AjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "id": "_63tEcZw0CKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "bEnhYTgH0CB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"Mansuba\"\n",
        "!git config --global user.email \"mansubatabassum9@gmail.com\""
      ],
      "metadata": {
        "id": "rwcvT2BmyhcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, Repository\n",
        "\n",
        "# Ensure you're logged in to Hugging Face Hub\n",
        "# This is already assumed to be done; if not, run `huggingface-cli login`\n",
        "\n",
        "# Define the model path and repo information\n",
        "model_path = '/content/final_clip_model.zip'  # Path to your model zip file\n",
        "repo_name = \"Mansuba/BanglaClip\"  # The repo where the model will be uploaded\n",
        "\n",
        "# Initialize the repository (clone from Hugging Face Hub)\n",
        "repo = Repository(local_dir='/content/BanglaClipRepo', clone_from=repo_name)\n",
        "\n",
        "# Step 1: Copy the model to the repo directory\n",
        "import shutil\n",
        "shutil.copy(model_path, '/content/BanglaClipRepo')  # Copy model file into the repo directory\n",
        "\n",
        "# Step 2: Push the model to Hugging Face Hub\n",
        "repo.push_to_hub(commit_message=\"Add final_clip_model.zip\")\n",
        "\n",
        "print(\"Model uploaded successfully to Hugging Face Hub!\")\n"
      ],
      "metadata": {
        "id": "JQn4Q96xyJxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the status of the git repo to ensure everything is up-to-date\n",
        "!git -C /content/BanglaClipRepo status\n",
        "\n",
        "# Push to Hugging Face Hub manually from the local repo\n",
        "!git -C /content/BanglaClipRepo push origin main\n"
      ],
      "metadata": {
        "id": "elavPbWFzi8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git -C /content/BanglaClipRepo status\n"
      ],
      "metadata": {
        "id": "coLMVYix0Fud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git -C /content/BanglaClipRepo add final_clip_model.zip\n"
      ],
      "metadata": {
        "id": "GatdNKFV0Jwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git -C /content/BanglaClipRepo commit -m \"Add final_clip_model.zip\"\n"
      ],
      "metadata": {
        "id": "wpzPK5vc0NoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git -C /content/BanglaClipRepo push origin main\n"
      ],
      "metadata": {
        "id": "mWH6Y9L90SWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Define the path for the model in Google Drive\n",
        "model_path = '/content/final_clip_model.zip'  # Path to the model zip file\n",
        "drive_folder = '/content/drive/MyDrive/Bangla Image dataset with caption'  # Target directory in Google Drive\n",
        "\n",
        "# Step 3: Create the target folder if it doesn't exist\n",
        "import os\n",
        "if not os.path.exists(drive_folder):\n",
        "    os.makedirs(drive_folder)\n",
        "\n",
        "# Step 4: Copy the model to the target folder\n",
        "shutil.copy(model_path, os.path.join(drive_folder, 'final_clip_model.zip'))\n",
        "\n",
        "print(f\"Model saved to Google Drive at: {os.path.join(drive_folder, 'final_clip_model.zip')}\")\n"
      ],
      "metadata": {
        "id": "xD3P7plz0f46"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}