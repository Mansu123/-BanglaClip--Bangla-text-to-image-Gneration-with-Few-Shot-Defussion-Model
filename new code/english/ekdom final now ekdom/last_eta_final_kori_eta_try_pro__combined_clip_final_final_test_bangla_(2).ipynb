{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUj_UmXU-z0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1d8880-7863-4562-d662-0bc2ee644a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining"
      ],
      "metadata": {
        "id": "2Fe9Bx7wOcHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFEBx9v_wC-",
        "outputId": "1d1faeb1-e4eb-4928-d40f-c50e71f61767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add get_lr function\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# First, update the CFG class with optimized parameters\n",
        "class CFG:\n",
        "    debug = False\n",
        "    # Dataset paths remain the same\n",
        "    image_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "    captions_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset'\n",
        "    image_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "    captions_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n",
        "    image_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/images'\n",
        "    captions_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/captions.json'\n",
        "\n",
        "    # Optimized training parameters\n",
        "    batch_size = 64  # Increased batch size\n",
        "    gradient_accumulation_steps = 2  # Reduced accumulation steps\n",
        "    num_workers = 4  # Increased workers\n",
        "    pin_memory = True\n",
        "    mixed_precision = True\n",
        "\n",
        "    # Optimized learning rates\n",
        "    image_encoder_lr = 2e-4  # Increased learning rate\n",
        "    text_encoder_lr = 2e-4   # Increased learning rate\n",
        "    head_lr = 5e-4          # Increased learning rate\n",
        "    weight_decay = 0.01\n",
        "\n",
        "    # Early stopping and scheduler settings\n",
        "    patience = 2\n",
        "    factor = 0.7\n",
        "    epochs = 3\n",
        "    warmup_ratio = 0.05\n",
        "\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 128  # Reduced max length\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 0.07  # Adjusted temperature\n",
        "    size = 224\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 200\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 1.0\n",
        "    size = 224\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1"
      ],
      "metadata": {
        "id": "alQ8syEyRrFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions():\n",
        "    \"\"\"Load captions with improved validation\"\"\"\n",
        "    captions_list = []\n",
        "\n",
        "    # Load Flickr8k dataset\n",
        "    try:\n",
        "        with open(os.path.join(CFG.captions_path1, 'BAN-Cap_captiondata.json'), 'r', encoding='utf-8') as f:\n",
        "            captions_data1 = json.load(f)\n",
        "\n",
        "        for entry in captions_data1:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id']).split('#')[0]\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Flickr8k dataset: {str(e)}\")\n",
        "\n",
        "    # Load BNATURE dataset\n",
        "    try:\n",
        "        with open(CFG.captions_path2, 'r', encoding='utf-8') as f:\n",
        "            captions_data2 = json.load(f)\n",
        "\n",
        "        for entry in captions_data2:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id'])\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BNATURE dataset: {str(e)}\")\n",
        "\n",
        "    # Load Bangla Lekha dataset with improved handling\n",
        "    try:\n",
        "        with open(CFG.captions_path3, 'r', encoding='utf-8') as f:\n",
        "            captions_data3 = json.load(f)\n",
        "\n",
        "        if isinstance(captions_data3, list):\n",
        "            for entry in captions_data3:\n",
        "                if isinstance(entry, dict) and 'filename' in entry and 'caption' in entry:\n",
        "                    filename = str(entry['filename'])\n",
        "                    caption = str(entry['caption'])\n",
        "                    if caption and filename:\n",
        "                        captions_list.append({\n",
        "                            \"image\": filename.strip(),\n",
        "                            \"caption\": caption.strip()\n",
        "                        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Bangla Lekha dataset: {str(e)}\")\n",
        "\n",
        "    df = pd.DataFrame(captions_list)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "    df['id'] = df.index // 5\n",
        "\n",
        "    print(f\"Loaded {len(df)} valid caption entries\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "pbY8J5qT7vjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions),\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.valid_indices = []\n",
        "        for idx in range(len(self.image_filenames)):\n",
        "            try:\n",
        "                image_found = False\n",
        "                for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                    if os.path.exists(os.path.join(path, self.image_filenames[idx])):\n",
        "                        image_found = True\n",
        "                        break\n",
        "\n",
        "                if image_found:\n",
        "                    self.valid_indices.append(idx)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} valid images out of {len(image_filenames)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            actual_idx = self.valid_indices[idx]\n",
        "\n",
        "            item = {\n",
        "                'input_ids': self.encoded_captions['input_ids'][actual_idx],\n",
        "                'attention_mask': self.encoded_captions['attention_mask'][actual_idx],\n",
        "            }\n",
        "\n",
        "            image_path = None\n",
        "            for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                if os.path.exists(os.path.join(path, self.image_filenames[actual_idx])):\n",
        "                    image_path = path\n",
        "                    break\n",
        "\n",
        "            if image_path is None:\n",
        "                raise FileNotFoundError(f\"Image {self.image_filenames[actual_idx]} not found in any path\")\n",
        "\n",
        "            image = cv2.imread(os.path.join(image_path, self.image_filenames[actual_idx]))\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load image: {self.image_filenames[actual_idx]}\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = self.transforms(image=image)['image']\n",
        "            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "            item['caption'] = self.captions[actual_idx]\n",
        "\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)"
      ],
      "metadata": {
        "id": "Ux2LXdjv8W1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    \"\"\"\n",
        "    Build data loaders with error handling\n",
        "    \"\"\"\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    try:\n",
        "        dataset = CLIPDataset(\n",
        "            image_filenames=dataframe[\"image\"].values,\n",
        "            captions=dataframe[\"caption\"].values,\n",
        "            tokenizer=tokenizer,\n",
        "            transforms=transforms\n",
        "        )\n",
        "\n",
        "        # Custom collate function to handle potential None values\n",
        "        def collate_fn(batch):\n",
        "            # Filter out None values\n",
        "            batch = [item for item in batch if item is not None]\n",
        "            if len(batch) == 0:\n",
        "                raise RuntimeError(\"Empty batch after filtering\")\n",
        "\n",
        "            return {\n",
        "                'image': torch.stack([item['image'] for item in batch]),\n",
        "                'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "                'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "                'caption': [item['caption'] for item in batch]\n",
        "            }\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=CFG.batch_size,\n",
        "            num_workers=CFG.num_workers,\n",
        "            shuffle=True if mode == \"train\" else False,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=True  # Drop incomplete batches\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building dataloader: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "nWuZvOjiuPS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG5GLPfx-0f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = AutoModel(config=AutoConfig.from_pretrained(model_name))\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "jtKuygbHRxip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbMomVsf-0lB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "R5SvBzV4wzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSqff5xX-0qA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_train_valid_dfs():\n",
        "    dataframe = load_captions()\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-D3J1fG-0sn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "OGhQqWJeHfVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "clFv516SDOoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "# Memory optimizations\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "# Configure PyTorch memory allocator\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.8,expandable_segments:True'\n"
      ],
      "metadata": {
        "id": "HXL2PKR3M5xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set memory and speed optimizations\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "# Configure PyTorch memory allocator\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'\n",
        "\n",
        "def compute_cosine_similarity(embeddings_a, embeddings_b, temperature=0.07):\n",
        "    # Process in chunks to save memory\n",
        "    chunk_size = 256\n",
        "    num_chunks = (embeddings_a.size(0) + chunk_size - 1) // chunk_size\n",
        "    similarity_chunks = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min((i + 1) * chunk_size, embeddings_a.size(0))\n",
        "        chunk_a = embeddings_a[start_idx:end_idx]\n",
        "\n",
        "        # Normalize chunks\n",
        "        chunk_a = torch.nn.functional.normalize(chunk_a, p=2, dim=-1)\n",
        "        chunk_b = torch.nn.functional.normalize(embeddings_b, p=2, dim=-1)\n",
        "\n",
        "        # Compute similarity for chunk\n",
        "        chunk_sim = torch.mm(chunk_a, chunk_b.t()) / temperature\n",
        "        similarity_chunks.append(chunk_sim)\n",
        "\n",
        "    return torch.cat(similarity_chunks, dim=0)\n",
        "\n",
        "def compute_recall_at_k(similarities, k):\n",
        "    argsort = torch.argsort(similarities, dim=-1, descending=True)\n",
        "    diagonal = torch.arange(similarities.size(0), device=similarities.device)\n",
        "    topk_indices = argsort[:, :k]\n",
        "    recall_at_k = (topk_indices == diagonal.view(-1, 1)).any(dim=-1).float().mean()\n",
        "    return recall_at_k.item()"
      ],
      "metadata": {
        "id": "KdN4_eBrfLpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import os\n",
        "from transformers import get_cosine_schedule_with_warmup, AutoTokenizer\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import os\n",
        "from transformers import get_cosine_schedule_with_warmup, AutoTokenizer\n",
        "\n",
        "def train(model, train_loader, optimizer, lr_scheduler, scaler, epoch):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "    similarity_meter = AvgMeter()\n",
        "    train_recall_meter = defaultdict(AvgMeter)\n",
        "\n",
        "    max_steps = len(train_loader) if epoch > 0 else len(train_loader) // 2\n",
        "    tqdm_object = tqdm(train_loader, total=max_steps)\n",
        "\n",
        "    image_embeddings_list = []\n",
        "    text_embeddings_list = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm_object):\n",
        "        if batch_idx >= max_steps:\n",
        "            break\n",
        "\n",
        "        batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "        accumulation_steps = 4 if epoch == 0 else 2\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            loss = model(batch)\n",
        "            with torch.no_grad():\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                similarity = F.cosine_similarity(\n",
        "                    image_embeddings.unsqueeze(1),\n",
        "                    text_embeddings.unsqueeze(0),\n",
        "                    dim=-1\n",
        "                ).mean()\n",
        "\n",
        "                image_embeddings_list.append(image_embeddings.cpu())\n",
        "                text_embeddings_list.append(text_embeddings.cpu())\n",
        "\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        loss_meter.update(loss.item() * accumulation_steps, batch[\"image\"].size(0))\n",
        "        similarity_meter.update(similarity.item(), batch[\"image\"].size(0))\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate and update train recalls periodically\n",
        "        if (batch_idx + 1) % 100 == 0 or batch_idx == max_steps - 1:\n",
        "            image_embeddings = torch.cat(image_embeddings_list)\n",
        "            text_embeddings = torch.cat(text_embeddings_list)\n",
        "\n",
        "            i2t_similarities = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "            t2i_similarities = i2t_similarities.t()\n",
        "\n",
        "            for k in [1, 5, 10]:\n",
        "                train_recall = compute_recall_at_k(i2t_similarities, k)\n",
        "                train_recall_meter[f'R@{k}'].update(train_recall, len(image_embeddings))\n",
        "\n",
        "            image_embeddings_list = []\n",
        "            text_embeddings_list = []\n",
        "\n",
        "        tqdm_object.set_postfix(\n",
        "            loss=f\"{loss_meter.avg:.4f}\",\n",
        "            similarity=f\"{similarity_meter.avg:.4f}\",\n",
        "            lr=f\"{get_lr(optimizer):.6f}\"\n",
        "        )\n",
        "\n",
        "    train_recalls = {k: v.avg for k, v in train_recall_meter.items()}\n",
        "    return loss_meter.avg, similarity_meter.avg, train_recalls\n",
        "\n",
        "def validate(model, valid_loader):\n",
        "    model.eval()\n",
        "    loss_meter = AvgMeter()\n",
        "    similarity_meter = AvgMeter()\n",
        "\n",
        "    max_val_steps = len(valid_loader) // 2\n",
        "\n",
        "    image_embeddings_list = []\n",
        "    text_embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(valid_loader)):\n",
        "            if batch_idx >= max_val_steps:\n",
        "                break\n",
        "\n",
        "            batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                loss = model(batch)\n",
        "\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                similarity = F.cosine_similarity(\n",
        "                    image_embeddings.unsqueeze(1),\n",
        "                    text_embeddings.unsqueeze(0),\n",
        "                    dim=-1\n",
        "                ).mean()\n",
        "\n",
        "                image_embeddings_list.append(image_embeddings.cpu())\n",
        "                text_embeddings_list.append(text_embeddings.cpu())\n",
        "\n",
        "            loss_meter.update(loss.item(), batch[\"image\"].size(0))\n",
        "            similarity_meter.update(similarity.item(), batch[\"image\"].size(0))\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    image_embeddings = torch.cat(image_embeddings_list)\n",
        "    text_embeddings = torch.cat(text_embeddings_list)\n",
        "\n",
        "    i2t_similarities = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "    t2i_similarities = i2t_similarities.t()\n",
        "\n",
        "    metrics = {\n",
        "        'val_similarity': similarity_meter.avg,\n",
        "        'val_loss': loss_meter.avg\n",
        "    }\n",
        "\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'image_to_text_recall@{k}'] = compute_recall_at_k(i2t_similarities, k)\n",
        "        metrics[f'text_to_image_recall@{k}'] = compute_recall_at_k(t2i_similarities, k)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def plot_training_progress(metrics_history, epoch):\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Plot 1: Losses\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(metrics_history['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(metrics_history['val_loss'], label='Val Loss', marker='x')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Validation Recalls\n",
        "    plt.subplot(3, 2, 2)\n",
        "    for k in [1, 5, 10]:\n",
        "        plt.plot(\n",
        "            [m[f'image_to_text_recall@{k}'] for m in metrics_history['val_metrics']],\n",
        "            label=f'Val R@{k}',\n",
        "            marker='o'\n",
        "        )\n",
        "    plt.title('Validation Image-to-Text Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: Training Recalls\n",
        "    plt.subplot(3, 2, 3)\n",
        "    for k in ['R@1', 'R@5', 'R@10']:\n",
        "        plt.plot(\n",
        "            [recalls[k] for recalls in metrics_history['train_recalls']],\n",
        "            label=f'Train {k}',\n",
        "            marker='o'\n",
        "        )\n",
        "    plt.title('Training Image-to-Text Recall')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 4: Similarities\n",
        "    plt.subplot(3, 2, 4)\n",
        "    plt.plot(metrics_history['train_similarity'], label='Train Similarity', marker='o')\n",
        "    plt.plot([m['val_similarity'] for m in metrics_history['val_metrics']],\n",
        "             label='Val Similarity', marker='x')\n",
        "    plt.title('Cosine Similarities')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Similarity')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 5: Learning Rate\n",
        "    plt.subplot(3, 2, 5)\n",
        "    plt.plot(metrics_history['learning_rates'], label='Learning Rate', marker='o')\n",
        "    plt.title('Learning Rate Progress')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 6: Train vs Val Recall Comparison\n",
        "    plt.subplot(3, 2, 6)\n",
        "    plt.plot(\n",
        "        [recalls['R@5'] for recalls in metrics_history['train_recalls']],\n",
        "        label='Train R@5',\n",
        "        marker='o'\n",
        "    )\n",
        "    plt.plot(\n",
        "        [m['image_to_text_recall@5'] for m in metrics_history['val_metrics']],\n",
        "        label='Val R@5',\n",
        "        marker='x'\n",
        "    )\n",
        "    plt.title('Train vs Validation R@5 Comparison')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall@5')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    plt.savefig(f'training_progress_epoch_{epoch+1}_{timestamp}.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    print(\"Starting optimized training...\")\n",
        "\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel(\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding\n",
        "    )\n",
        "\n",
        "    # Load checkpoint\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_recall = 0\n",
        "\n",
        "    checkpoint_path = '/content/best_model_epoch_8.pth'\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
        "        best_recall = checkpoint.get('best_recall', 0)\n",
        "        start_epoch = checkpoint.get('epoch', -1) + 1\n",
        "\n",
        "        del checkpoint\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    model = model.to(CFG.device)\n",
        "\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr * 0.1},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr * 0.1},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(),\n",
        "            model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr}\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=CFG.weight_decay)\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    num_training_steps = len(train_loader) * (CFG.epochs - start_epoch)\n",
        "    num_warmup_steps = int(num_training_steps * 0.15)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "    metrics_history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_similarity': [],\n",
        "        'train_recalls': [],\n",
        "        'val_metrics': [],\n",
        "        'learning_rates': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(start_epoch, CFG.epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "\n",
        "        train_loss, train_similarity, train_recalls = train(\n",
        "            model, train_loader, optimizer, scheduler, scaler, epoch\n",
        "        )\n",
        "\n",
        "        val_metrics = validate(model, valid_loader)\n",
        "\n",
        "        # Update metrics history\n",
        "        metrics_history['train_loss'].append(train_loss)\n",
        "        metrics_history['val_loss'].append(val_metrics['val_loss'])\n",
        "        metrics_history['train_similarity'].append(train_similarity)\n",
        "        metrics_history['train_recalls'].append(train_recalls)\n",
        "        metrics_history['val_metrics'].append(val_metrics)\n",
        "        metrics_history['learning_rates'].append(get_lr(optimizer))\n",
        "\n",
        "        # Plot progress\n",
        "        plot_training_progress(metrics_history, epoch)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_metrics['val_loss']:.4f}\")\n",
        "        print(f\"Train Similarity: {train_similarity:.4f}\")\n",
        "        print(f\"Val Similarity: {val_metrics['val_similarity']:.4f}\")\n",
        "        print(\"\\nTrain Recalls:\")\n",
        "        for k, v in train_recalls.items():\n",
        "            print(f\"Train {k}: {v:.4f}\")\n",
        "        print(\"\\nValidation Recalls:\")\n",
        "        for k in [1, 5, 10]:\n",
        "            print(f\"Val R@{k}: {val_metrics[f'image_to_text_recall@{k}']:.4f}\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        current_recall = val_metrics['image_to_text_recall@5']\n",
        "        if (epoch + 1) % 5 == 0 or val_metrics['val_loss'] < best_val_loss or current_recall > best_recall:\n",
        "            best_val_loss = min(val_metrics['val_loss'], best_val_loss)\n",
        "            best_recall = max(current_recall, best_recall)\n",
        "\n",
        "            save_path = f'model_epoch_{epoch+1}.pth'\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'best_recall': best_recall,\n",
        "                'metrics_history': metrics_history\n",
        "            }, save_path)\n",
        "\n",
        "            print(f\"Saved model checkpoint - Val Loss: {best_val_loss:.4f}, R@5: {best_recall:.4f}\")\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "            if early_stopping_counter >= CFG.patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqFxXMV6xOLi",
        "outputId": "d38a1aad-66a0-4bc7-ef27-8c8be240910a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting optimized training...\n",
            "Loaded 88641 valid caption entries\n",
            "Found 70919 valid images out of 70919\n",
            "Found 17722 valid images out of 17722\n",
            "Loading checkpoint from /content/best_model_epoch_8.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-339dcac054de>:272: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the Google Drive path for saving the model\n",
        "save_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombinedfinalnowok22prookafinal30000.pt\"\n",
        "\n",
        "# Define your model class\n",
        "model = CLIPModel().to(CFG.device)\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint_path = \"/content/model_epoch_10.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, weights_only=True)  # Add weights_only=True to address the warning\n",
        "\n",
        "# Extract just the model state dict from the checkpoint\n",
        "if \"model_state_dict\" in checkpoint:\n",
        "    # If the checkpoint contains the full training state\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "else:\n",
        "    # If the checkpoint contains only the model state\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "# Save the model's state dictionary to the specified Google Drive path\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully at: {save_path}\")"
      ],
      "metadata": {
        "id": "UjwA--gvVzF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdee983e-1865-42e1-8c14-01a2906583fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully at: /content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombinedfinalnowok22prookafinal30000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJxU0NebXSLj"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config=None, # Add a default config argument\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # If config is None, use CFG as default config\n",
        "        if config is None:\n",
        "            config = CFG\n",
        "\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "id": "K4rNY5FrFluV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def load_model(model_path):\n",
        "    \"\"\"Load the CLIP model with proper error handling\"\"\"\n",
        "    try:\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        model = CLIPModel().to(CFG.device)\n",
        "\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
        "\n",
        "        checkpoint = torch.load(model_path, map_location=CFG.device)\n",
        "\n",
        "        if \"model_state_dict\" in checkpoint:\n",
        "            model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint)\n",
        "\n",
        "        print(\"Model loaded successfully!\")\n",
        "        return model\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def load_and_verify_image(image_path):\n",
        "    \"\"\"Safely load and verify an image\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Warning: Image not found at {image_path}\")\n",
        "            return None\n",
        "        img = Image.open(image_path)\n",
        "        img = img.convert('RGB')\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image {image_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_image_embeddings(dataframe, model_path):\n",
        "    \"\"\"Generate image embeddings with proper error handling\"\"\"\n",
        "    print(\"Initializing tokenizer and model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    model = load_model(model_path)\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Setting up data loader...\")\n",
        "    transforms = get_transforms(mode=\"valid\")\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    print(\"Generating embeddings...\")\n",
        "    image_embeddings_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Processing images\"):\n",
        "            images = batch[\"image\"].to(CFG.device)\n",
        "            image_features = model.image_encoder(images)\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            image_embeddings_list.append(image_embeddings)\n",
        "\n",
        "    image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "    print(f\"Generated embeddings shape: {image_embeddings.shape}\")\n",
        "    return model, image_embeddings\n",
        "\n",
        "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
        "    \"\"\"Find image matches for a given text query\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    encoded_query = tokenizer(\n",
        "        query,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=CFG.max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(CFG.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=encoded_query[\"input_ids\"],\n",
        "            attention_mask=encoded_query[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "    dot_similarity = image_embeddings_n @ text_embeddings_n.t()\n",
        "\n",
        "    values, indices = torch.topk(dot_similarity.squeeze(1), n)\n",
        "    return indices.cpu().numpy(), values.cpu().numpy()\n",
        "\n",
        "def display_results(query, image_paths, indices, similarities):\n",
        "    \"\"\"Display search results in a grid\"\"\"\n",
        "    n = len(indices)\n",
        "    ncols = 3\n",
        "    nrows = (n + ncols - 1) // ncols\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(15, 5 * nrows))\n",
        "\n",
        "    for idx in range(n):\n",
        "        plt.subplot(nrows, ncols, idx + 1)\n",
        "\n",
        "        img_path = image_paths[indices[idx]]\n",
        "        img = load_and_verify_image(img_path)\n",
        "\n",
        "        if img is not None:\n",
        "            plt.imshow(img)\n",
        "            similarity_score = similarities[idx] * 100\n",
        "            plt.title(f'Similarity: {similarity_score:.2f}%', pad=10)\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, 'Image not found', ha='center', va='center')\n",
        "\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.suptitle(f'Results for query: {query}', fontsize=16, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        print(\"Starting image search system...\")\n",
        "\n",
        "        # Load data\n",
        "        print(\"Loading dataset...\")\n",
        "        _, valid_df = make_train_valid_dfs()\n",
        "\n",
        "        # Set model path\n",
        "        model_path = \"/content/model_epoch_10.pth\"\n",
        "        if not os.path.exists(model_path):\n",
        "            raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
        "\n",
        "        # Generate embeddings\n",
        "        model, image_embeddings = get_image_embeddings(valid_df, model_path)\n",
        "\n",
        "        # Define test queries\n",
        "        queries = [\n",
        "            \" \",  # Royal Bengal Tiger\n",
        "            \" \",  # Bangladesh National Flag\n",
        "            \" \",  # Dhaka's Rickshaw\n",
        "        ]\n",
        "\n",
        "        # Process each query\n",
        "        for query in queries:\n",
        "            print(f\"\\nProcessing query: {query}\")\n",
        "\n",
        "            indices, similarities = find_matches(\n",
        "                model=model,\n",
        "                image_embeddings=image_embeddings,\n",
        "                query=query,\n",
        "                image_filenames=valid_df[\"image\"].values,\n",
        "                n=9\n",
        "            )\n",
        "\n",
        "            display_results(\n",
        "                query=query,\n",
        "                image_paths=valid_df[\"image\"].values,\n",
        "                indices=indices,\n",
        "                similarities=similarities\n",
        "            )\n",
        "\n",
        "            print(f\"\\nResults for: {query}\")\n",
        "            print(f\"Average similarity: {np.mean(similarities)*100:.2f}%\")\n",
        "            print(f\"Max similarity: {np.max(similarities)*100:.2f}%\")\n",
        "\n",
        "            # Small delay between displays\n",
        "            plt.pause(1)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"File not found error: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set only basic matplotlib settings\n",
        "    plt.rcParams['figure.figsize'] = (15, 10)\n",
        "    plt.rcParams['axes.grid'] = False\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iUiJU87MEAiT",
        "outputId": "8c01fc6d-a48e-4721-e0ec-f4f96187e919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-82-8d093575bb4b>:135: UserWarning: Glyph 2466 (\\N{BENGALI LETTER DDHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-82-8d093575bb4b>:135: UserWarning: Glyph 2495 (\\N{BENGALI VOWEL SIGN I}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "<ipython-input-82-8d093575bb4b>:135: UserWarning: Glyph 2486 (\\N{BENGALI LETTER SHA}) missing from font(s) DejaVu Sans.\n",
            "  plt.tight_layout()\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2466 (\\N{BENGALI LETTER DDHA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2495 (\\N{BENGALI VOWEL SIGN I}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 2486 (\\N{BENGALI LETTER SHA}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1500 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install huggingface_hub transformers\n",
        "\n",
        "# Step 1: Configure Git credentials\n",
        "!git config --global user.email \"mansubatabassum9@gmailcom\"\n",
        "!git config --global user.name \"Tabassum\"\n",
        "\n",
        "# Step 2: Define your Hugging Face token\n",
        "import os\n",
        "from huggingface_hub import login, HfApi, Repository\n",
        "\n",
        "# Replace this with your actual token from huggingface.co/settings/tokens\n",
        "HF_TOKEN = \"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\"  # Replace this with your actual token!\n",
        "\n",
        "# Login to Hugging Face\n",
        "try:\n",
        "    login(token=HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "except Exception as e:\n",
        "    print(f\"Login failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 3: Repository setup\n",
        "repo_name = \"BanglaCLIP13\"\n",
        "repo_id = \"Mansuba/BanglaCLIP13\"\n",
        "local_model_path = \"/content/model_epoch_10.pth\"\n",
        "\n",
        "# Clean up existing repository\n",
        "if os.path.exists(repo_name):\n",
        "    shutil.rmtree(repo_name)\n",
        "    print(f\"Cleaned up existing {repo_name} directory\")\n",
        "\n",
        "# Initialize API\n",
        "api = HfApi()\n",
        "\n",
        "# Create or verify repository\n",
        "try:\n",
        "    api.create_repo(\n",
        "        repo_id,\n",
        "        private=False,\n",
        "        exist_ok=True,\n",
        "        token=HF_TOKEN\n",
        "    )\n",
        "    print(f\"Repository {repo_id} is ready\")\n",
        "except Exception as e:\n",
        "    print(f\"Repository setup error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Clone repository\n",
        "try:\n",
        "    repo = Repository(\n",
        "        local_dir=repo_name,\n",
        "        clone_from=f\"https://huggingface.co/{repo_id}\",\n",
        "        use_auth_token=HF_TOKEN,\n",
        "        git_user=\"Tabassum\",\n",
        "        git_email=\"mansubatabassum9@gmailcom\"\n",
        "    )\n",
        "    print(\"Repository cloned successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Cloning error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Step 4: Prepare model files\n",
        "config = {\n",
        "    \"architectures\": [\"CLIPModel\"],\n",
        "    \"model_type\": \"clip\",\n",
        "    \"hidden_size\": 768,\n",
        "    \"intermediate_size\": 3072,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"num_hidden_layers\": 12\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Save config\n",
        "    with open(os.path.join(repo_name, \"config.json\"), \"w\") as f:\n",
        "        json.dump(config, f)\n",
        "    print(\"Config file created\")\n",
        "\n",
        "    # Copy model file\n",
        "    model_name = \"banglaclip_model_epoch_10.pth\"\n",
        "    target_model_path = os.path.join(repo_name, model_name)\n",
        "    shutil.copy(local_model_path, target_model_path)\n",
        "    print(\"Model file copied\")\n",
        "\n",
        "    # Save tokenizer config\n",
        "    tokenizer_config = {\n",
        "        \"model_max_length\": 77,\n",
        "        \"padding_side\": \"right\",\n",
        "        \"truncation_side\": \"right\"\n",
        "    }\n",
        "    with open(os.path.join(repo_name, \"tokenizer_config.json\"), \"w\") as f:\n",
        "        json.dump(tokenizer_config, f)\n",
        "    print(\"Tokenizer config created\")\n",
        "\n",
        "    # Commit and push\n",
        "    repo.git_add(\".\")\n",
        "    repo.git_commit(\"Upload model and configs\")\n",
        "    repo.git_push()\n",
        "    print(\"Successfully pushed to Hugging Face Hub!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during file operations: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "9f9eb9a854ab472087366fa576120960",
            "19593e9c486a47f1a49db86955e87ff9",
            "f67fd79f15954673bd72124d8d200dfc",
            "98a7fef5e8834af1bf0ec82348993eec",
            "33944bd37b7642399e022c8530d1db39",
            "c5dbb42bf43943939021aa071917e1ed",
            "9aabde43a8ca485eba2ee123f951336e",
            "09e467d3da0246549335c67cec150907",
            "847449a23a4f45d1b7d4bae4e04c67bd",
            "f6ecad91a69e42e7bfbcca92022645d2",
            "f6784b2774e34d27aa3eb14bdcfd145b"
          ]
        },
        "id": "bm6OjK3bB4Hl",
        "outputId": "0ea90a15-aa21-4a68-83e9-c4c65dc4282d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Cloning https://huggingface.co/Mansuba/BanglaCLIP13 into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/Mansuba/BanglaCLIP13 into local empty directory.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload file banglaclip_model_epoch_10.pth:   0%|          | 1.00/1.50G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f9eb9a854ab472087366fa576120960"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "To https://huggingface.co/Mansuba/BanglaCLIP13\n",
            "   c388d63..c39e4e8  main -> main\n",
            "\n",
            "WARNING:huggingface_hub.repository:To https://huggingface.co/Mansuba/BanglaCLIP13\n",
            "   c388d63..c39e4e8  main -> main\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DCWGvZWvDBxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2GjVYOxVDBuL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f9eb9a854ab472087366fa576120960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19593e9c486a47f1a49db86955e87ff9",
              "IPY_MODEL_f67fd79f15954673bd72124d8d200dfc",
              "IPY_MODEL_98a7fef5e8834af1bf0ec82348993eec"
            ],
            "layout": "IPY_MODEL_33944bd37b7642399e022c8530d1db39"
          }
        },
        "19593e9c486a47f1a49db86955e87ff9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c5dbb42bf43943939021aa071917e1ed",
            "placeholder": "",
            "style": "IPY_MODEL_9aabde43a8ca485eba2ee123f951336e",
            "value": "Uploadfilebanglaclip_model_epoch_10.pth:100%"
          }
        },
        "f67fd79f15954673bd72124d8d200dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09e467d3da0246549335c67cec150907",
            "max": 1613411232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_847449a23a4f45d1b7d4bae4e04c67bd",
            "value": 1613411232
          }
        },
        "98a7fef5e8834af1bf0ec82348993eec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ecad91a69e42e7bfbcca92022645d2",
            "placeholder": "",
            "style": "IPY_MODEL_f6784b2774e34d27aa3eb14bdcfd145b",
            "value": "1.50G/1.50G[01:10&lt;00:00,31.0MB/s]"
          }
        },
        "33944bd37b7642399e022c8530d1db39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5dbb42bf43943939021aa071917e1ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aabde43a8ca485eba2ee123f951336e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09e467d3da0246549335c67cec150907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "847449a23a4f45d1b7d4bae4e04c67bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6ecad91a69e42e7bfbcca92022645d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6784b2774e34d27aa3eb14bdcfd145b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}