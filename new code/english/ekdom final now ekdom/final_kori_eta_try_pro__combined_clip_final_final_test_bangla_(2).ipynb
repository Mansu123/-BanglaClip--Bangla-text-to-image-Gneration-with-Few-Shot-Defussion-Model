{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WUj_UmXU-z0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1d8880-7863-4562-d662-0bc2ee644a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining"
      ],
      "metadata": {
        "id": "2Fe9Bx7wOcHJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFEBx9v_wC-",
        "outputId": "1d1faeb1-e4eb-4928-d40f-c50e71f61767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add get_lr function\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# First, update the CFG class with optimized parameters\n",
        "class CFG:\n",
        "    debug = False\n",
        "    # Dataset paths remain the same\n",
        "    image_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "    captions_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset'\n",
        "    image_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "    captions_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n",
        "    image_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/images'\n",
        "    captions_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/captions.json'\n",
        "\n",
        "    # Optimized training parameters\n",
        "    batch_size = 64  # Increased batch size\n",
        "    gradient_accumulation_steps = 2  # Reduced accumulation steps\n",
        "    num_workers = 4  # Increased workers\n",
        "    pin_memory = True\n",
        "    mixed_precision = True\n",
        "\n",
        "    # Optimized learning rates\n",
        "    image_encoder_lr = 2e-4  # Increased learning rate\n",
        "    text_encoder_lr = 2e-4   # Increased learning rate\n",
        "    head_lr = 5e-4          # Increased learning rate\n",
        "    weight_decay = 0.01\n",
        "\n",
        "    # Early stopping and scheduler settings\n",
        "    patience = 2\n",
        "    factor = 0.7\n",
        "    epochs = 10\n",
        "    warmup_ratio = 0.05\n",
        "\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 128  # Reduced max length\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 0.07  # Adjusted temperature\n",
        "    size = 224\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 200\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 1.0\n",
        "    size = 224\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1"
      ],
      "metadata": {
        "id": "alQ8syEyRrFF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions():\n",
        "    \"\"\"Load captions with improved validation\"\"\"\n",
        "    captions_list = []\n",
        "\n",
        "    # Load Flickr8k dataset\n",
        "    try:\n",
        "        with open(os.path.join(CFG.captions_path1, 'BAN-Cap_captiondata.json'), 'r', encoding='utf-8') as f:\n",
        "            captions_data1 = json.load(f)\n",
        "\n",
        "        for entry in captions_data1:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id']).split('#')[0]\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Flickr8k dataset: {str(e)}\")\n",
        "\n",
        "    # Load BNATURE dataset\n",
        "    try:\n",
        "        with open(CFG.captions_path2, 'r', encoding='utf-8') as f:\n",
        "            captions_data2 = json.load(f)\n",
        "\n",
        "        for entry in captions_data2:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id'])\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BNATURE dataset: {str(e)}\")\n",
        "\n",
        "    # Load Bangla Lekha dataset with improved handling\n",
        "    try:\n",
        "        with open(CFG.captions_path3, 'r', encoding='utf-8') as f:\n",
        "            captions_data3 = json.load(f)\n",
        "\n",
        "        if isinstance(captions_data3, list):\n",
        "            for entry in captions_data3:\n",
        "                if isinstance(entry, dict) and 'filename' in entry and 'caption' in entry:\n",
        "                    filename = str(entry['filename'])\n",
        "                    caption = str(entry['caption'])\n",
        "                    if caption and filename:\n",
        "                        captions_list.append({\n",
        "                            \"image\": filename.strip(),\n",
        "                            \"caption\": caption.strip()\n",
        "                        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Bangla Lekha dataset: {str(e)}\")\n",
        "\n",
        "    df = pd.DataFrame(captions_list)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "    df['id'] = df.index // 5\n",
        "\n",
        "    print(f\"Loaded {len(df)} valid caption entries\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "pbY8J5qT7vjL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions),\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.valid_indices = []\n",
        "        for idx in range(len(self.image_filenames)):\n",
        "            try:\n",
        "                image_found = False\n",
        "                for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                    if os.path.exists(os.path.join(path, self.image_filenames[idx])):\n",
        "                        image_found = True\n",
        "                        break\n",
        "\n",
        "                if image_found:\n",
        "                    self.valid_indices.append(idx)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} valid images out of {len(image_filenames)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            actual_idx = self.valid_indices[idx]\n",
        "\n",
        "            item = {\n",
        "                'input_ids': self.encoded_captions['input_ids'][actual_idx],\n",
        "                'attention_mask': self.encoded_captions['attention_mask'][actual_idx],\n",
        "            }\n",
        "\n",
        "            image_path = None\n",
        "            for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                if os.path.exists(os.path.join(path, self.image_filenames[actual_idx])):\n",
        "                    image_path = path\n",
        "                    break\n",
        "\n",
        "            if image_path is None:\n",
        "                raise FileNotFoundError(f\"Image {self.image_filenames[actual_idx]} not found in any path\")\n",
        "\n",
        "            image = cv2.imread(os.path.join(image_path, self.image_filenames[actual_idx]))\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load image: {self.image_filenames[actual_idx]}\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = self.transforms(image=image)['image']\n",
        "            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "            item['caption'] = self.captions[actual_idx]\n",
        "\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)"
      ],
      "metadata": {
        "id": "Ux2LXdjv8W1O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    \"\"\"\n",
        "    Build data loaders with error handling\n",
        "    \"\"\"\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    try:\n",
        "        dataset = CLIPDataset(\n",
        "            image_filenames=dataframe[\"image\"].values,\n",
        "            captions=dataframe[\"caption\"].values,\n",
        "            tokenizer=tokenizer,\n",
        "            transforms=transforms\n",
        "        )\n",
        "\n",
        "        # Custom collate function to handle potential None values\n",
        "        def collate_fn(batch):\n",
        "            # Filter out None values\n",
        "            batch = [item for item in batch if item is not None]\n",
        "            if len(batch) == 0:\n",
        "                raise RuntimeError(\"Empty batch after filtering\")\n",
        "\n",
        "            return {\n",
        "                'image': torch.stack([item['image'] for item in batch]),\n",
        "                'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "                'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "                'caption': [item['caption'] for item in batch]\n",
        "            }\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=CFG.batch_size,\n",
        "            num_workers=CFG.num_workers,\n",
        "            shuffle=True if mode == \"train\" else False,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=True  # Drop incomplete batches\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building dataloader: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "nWuZvOjiuPS4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iG5GLPfx-0f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = AutoModel(config=AutoConfig.from_pretrained(model_name))\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "jtKuygbHRxip"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YbMomVsf-0lB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "        ])\n",
        "    else:\n",
        "        return A.Compose([\n",
        "            A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "            A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "        ])\n"
      ],
      "metadata": {
        "id": "R5SvBzV4wzR2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "FSqff5xX-0qA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_train_valid_dfs():\n",
        "    dataframe = load_captions()\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A-D3J1fG-0sn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "OGhQqWJeHfVn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "clFv516SDOoY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "# Memory optimizations\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "# Configure PyTorch memory allocator\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.8,expandable_segments:True'\n"
      ],
      "metadata": {
        "id": "HXL2PKR3M5xD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set memory and speed optimizations\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "# Configure PyTorch memory allocator\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'\n",
        "\n",
        "def compute_cosine_similarity(embeddings_a, embeddings_b, temperature=0.07):\n",
        "    # Process in chunks to save memory\n",
        "    chunk_size = 256\n",
        "    num_chunks = (embeddings_a.size(0) + chunk_size - 1) // chunk_size\n",
        "    similarity_chunks = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min((i + 1) * chunk_size, embeddings_a.size(0))\n",
        "        chunk_a = embeddings_a[start_idx:end_idx]\n",
        "\n",
        "        # Normalize chunks\n",
        "        chunk_a = F.normalize(chunk_a, p=2, dim=-1)\n",
        "        chunk_b = F.normalize(embeddings_b, p=2, dim=-1)\n",
        "\n",
        "        # Compute similarity for chunk\n",
        "        chunk_sim = torch.mm(chunk_a, chunk_b.t()) / temperature\n",
        "        similarity_chunks.append(chunk_sim)\n",
        "\n",
        "    return torch.cat(similarity_chunks, dim=0)\n",
        "\n",
        "def compute_recall_at_k(similarities, k):\n",
        "    argsort = torch.argsort(similarities, dim=-1, descending=True)\n",
        "    diagonal = torch.arange(similarities.size(0), device=similarities.device)\n",
        "    topk_indices = argsort[:, :k]\n",
        "    recall_at_k = (topk_indices == diagonal.view(-1, 1)).any(dim=-1).float().mean()\n",
        "    return recall_at_k.item()\n",
        "\n",
        "def compute_precision_at_k(similarities, k):\n",
        "    \"\"\"\n",
        "    Compute precision@k for the similarities matrix\n",
        "    \"\"\"\n",
        "    argsort = torch.argsort(similarities, dim=-1, descending=True)\n",
        "    diagonal = torch.arange(similarities.size(0), device=similarities.device)\n",
        "    topk_indices = argsort[:, :k]\n",
        "\n",
        "    # Count correct matches in top-k\n",
        "    correct = (topk_indices == diagonal.view(-1, 1)).float().sum(dim=1)\n",
        "\n",
        "    # Precision is number of correct matches divided by k\n",
        "    precision_at_k = (correct / k).mean()\n",
        "\n",
        "    return precision_at_k.item()\n",
        "\n",
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Compute and store the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    \"\"\"\n",
        "    Get the current learning rate from the optimizer\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']"
      ],
      "metadata": {
        "id": "_cD13RXKRNme"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, lr_scheduler, scaler, epoch):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "    similarity_meter = AvgMeter()\n",
        "\n",
        "    # Add meters for recall tracking during training\n",
        "    recall_meters = {k: AvgMeter() for k in [1, 5, 10]}\n",
        "\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "\n",
        "    # Store embeddings for recall calculation\n",
        "    image_embeddings_list = []\n",
        "    text_embeddings_list = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm_object):\n",
        "        batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        accumulation_steps = 2\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            loss = model(batch)\n",
        "\n",
        "            # Calculate embeddings and similarities\n",
        "            with torch.no_grad():\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                # Store embeddings for recall calculation\n",
        "                image_embeddings_list.append(image_embeddings.detach().cpu())\n",
        "                text_embeddings_list.append(text_embeddings.detach().cpu())\n",
        "\n",
        "                similarity = F.cosine_similarity(\n",
        "                    image_embeddings.unsqueeze(1),\n",
        "                    text_embeddings.unsqueeze(0),\n",
        "                    dim=-1\n",
        "                ).mean()\n",
        "\n",
        "            loss = loss / accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        loss_meter.update(loss.item() * accumulation_steps, batch[\"image\"].size(0))\n",
        "        similarity_meter.update(similarity.item(), batch[\"image\"].size(0))\n",
        "\n",
        "        # Calculate recall metrics every 100 batches\n",
        "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
        "            image_embeddings = torch.cat(image_embeddings_list)\n",
        "            text_embeddings = torch.cat(text_embeddings_list)\n",
        "\n",
        "            i2t_similarities = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "            t2i_similarities = i2t_similarities.t()\n",
        "\n",
        "            for k in [1, 5, 10]:\n",
        "                recall = compute_recall_at_k(i2t_similarities, k)\n",
        "                recall_meters[k].update(recall, image_embeddings.size(0))\n",
        "\n",
        "            # Clear lists to save memory\n",
        "            image_embeddings_list = []\n",
        "            text_embeddings_list = []\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Fixed the syntax error in the postfix dictionary\n",
        "        tqdm_object.set_postfix({\n",
        "            'loss': f\"{loss_meter.avg:.4f}\",\n",
        "            'sim': f\"{similarity_meter.avg:.4f}\",\n",
        "            'recall_5': f\"{recall_meters[5].avg:.4f}\",  # Changed from r@5 to recall_5\n",
        "            'lr': f\"{get_lr(optimizer):.6f}\"\n",
        "        })\n",
        "\n",
        "    train_recalls = {k: meter.avg for k, meter in recall_meters.items()}\n",
        "    return loss_meter.avg, similarity_meter.avg, train_recalls\n",
        "\n",
        "def validate(model, valid_loader):\n",
        "    model.eval()\n",
        "    loss_meter = AvgMeter()\n",
        "    similarity_meter = AvgMeter()\n",
        "    precision_meters = {k: AvgMeter() for k in [1, 5, 10]}\n",
        "\n",
        "    image_embeddings_list = []\n",
        "    text_embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(tqdm(valid_loader)):\n",
        "            batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                loss = model(batch)\n",
        "\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                similarity = F.cosine_similarity(\n",
        "                    image_embeddings.unsqueeze(1),\n",
        "                    text_embeddings.unsqueeze(0),\n",
        "                    dim=-1\n",
        "                ).mean()\n",
        "\n",
        "                image_embeddings_list.append(image_embeddings.cpu())\n",
        "                text_embeddings_list.append(text_embeddings.cpu())\n",
        "\n",
        "            loss_meter.update(loss.item(), batch[\"image\"].size(0))\n",
        "            similarity_meter.update(similarity.item(), batch[\"image\"].size(0))\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    image_embeddings = torch.cat(image_embeddings_list)\n",
        "    text_embeddings = torch.cat(text_embeddings_list)\n",
        "\n",
        "    i2t_similarities = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "    t2i_similarities = i2t_similarities.t()\n",
        "\n",
        "    metrics = {\n",
        "        'val_similarity': similarity_meter.avg,\n",
        "        'val_loss': loss_meter.avg\n",
        "    }\n",
        "\n",
        "    for k in [1, 5, 10]:\n",
        "        recall_i2t = compute_recall_at_k(i2t_similarities, k)\n",
        "        recall_t2i = compute_recall_at_k(t2i_similarities, k)\n",
        "        precision_i2t = compute_precision_at_k(i2t_similarities, k)\n",
        "        precision_t2i = compute_precision_at_k(t2i_similarities, k)\n",
        "\n",
        "        metrics[f'image_to_text_recall@{k}'] = recall_i2t\n",
        "        metrics[f'text_to_image_recall@{k}'] = recall_t2i\n",
        "        metrics[f'image_to_text_precision@{k}'] = precision_i2t\n",
        "        metrics[f'text_to_image_precision@{k}'] = precision_t2i\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def plot_training_progress(metrics_history, epoch):\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Plot 1: Losses\n",
        "    plt.subplot(3, 2, 1)\n",
        "    plt.plot(metrics_history['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(metrics_history['val_loss'], label='Val Loss', marker='x')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 2: Training Recalls\n",
        "    plt.subplot(3, 2, 2)\n",
        "    for k in [1, 5, 10]:\n",
        "        plt.plot(\n",
        "            metrics_history['train_recalls'][f'R@{k}'],\n",
        "            label=f'Train R@{k}',\n",
        "            marker='o'\n",
        "        )\n",
        "    plt.title('Training Recalls')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 3: Validation Recalls\n",
        "    plt.subplot(3, 2, 3)\n",
        "    for k in [1, 5, 10]:\n",
        "        plt.plot(\n",
        "            [m[f'image_to_text_recall@{k}'] for m in metrics_history['val_metrics']],\n",
        "            label=f'Val R@{k}',\n",
        "            marker='o'\n",
        "        )\n",
        "    plt.title('Validation Recalls')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 4: Precision\n",
        "    plt.subplot(3, 2, 4)\n",
        "    for k in [1, 5, 10]:\n",
        "        plt.plot(\n",
        "            [m[f'image_to_text_precision@{k}'] for m in metrics_history['val_metrics']],\n",
        "            label=f'P@{k}',\n",
        "            marker='o'\n",
        "        )\n",
        "    plt.title('Image-to-Text Precision')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 5: Similarities\n",
        "    plt.subplot(3, 2, 5)\n",
        "    plt.plot(metrics_history['train_similarity'], label='Train Similarity', marker='o')\n",
        "    plt.plot([m['val_similarity'] for m in metrics_history['val_metrics']],\n",
        "             label='Val Similarity', marker='x')\n",
        "    plt.title('Cosine Similarities')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Similarity')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot 6: Learning Rate\n",
        "    plt.subplot(3, 2, 6)\n",
        "    plt.plot(metrics_history['learning_rates'], label='Learning Rate', marker='o')\n",
        "    plt.title('Learning Rate Progress')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.yscale('log')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_progress_epoch_{epoch+1}.png')\n",
        "    plt.close()\n",
        "\n",
        "def main():\n",
        "    print(\"Starting optimized training...\")\n",
        "\n",
        "    # Set memory optimization flags\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel(\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding\n",
        "    )\n",
        "\n",
        "    # Load checkpoint with memory optimization\n",
        "    checkpoint = torch.load('/content/best_model_epoch_8.pth', map_location='cpu', weights_only=True)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    best_recall = checkpoint['best_recall']\n",
        "    del checkpoint['model_state_dict']  # Free memory\n",
        "\n",
        "    # Move model to GPU after loading weights\n",
        "    model = model.to(CFG.device)\n",
        "\n",
        "    # Apply memory optimizations to model\n",
        "    model.half()  # Convert to half precision\n",
        "    for param in model.parameters():\n",
        "        param.grad = None  # Clear gradients\n",
        "\n",
        "    # Modified optimizer setup with memory-efficient settings\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr * 0.5},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr * 0.5},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(),\n",
        "            model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr * 0.7}\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=CFG.weight_decay)\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    num_training_steps = len(train_loader) * CFG.epochs\n",
        "    num_warmup_steps = int(num_training_steps * 0.05)\n",
        "\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    # Modified metrics history initialization\n",
        "    metrics_history = checkpoint.get('metrics_history', {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_similarity': [],\n",
        "        'val_metrics': [],\n",
        "        'learning_rates': [],\n",
        "        'train_recalls': {k: [] for k in [1, 5, 10]}\n",
        "    })\n",
        "\n",
        "    del checkpoint  # Free memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "\n",
        "        # Clear cache before each epoch\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Training with recall tracking\n",
        "        train_loss, train_similarity, train_recalls = train(\n",
        "            model, train_loader, optimizer, scheduler, scaler, epoch\n",
        "        )\n",
        "\n",
        "        # Clear cache before validation\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Validation\n",
        "        val_metrics = validate(model, valid_loader)\n",
        "\n",
        "        # Update metrics history\n",
        "        metrics_history['train_loss'].append(train_loss)\n",
        "        metrics_history['val_loss'].append(val_metrics['val_loss'])\n",
        "        metrics_history['train_similarity'].append(train_similarity)\n",
        "        metrics_history['val_metrics'].append(val_metrics)\n",
        "        metrics_history['learning_rates'].append(get_lr(optimizer))\n",
        "\n",
        "        for k in [1, 5, 10]:\n",
        "            metrics_history['train_recalls'][k].append(train_recalls[k])\n",
        "\n",
        "        # Plot progress\n",
        "        plot_training_progress(metrics_history, epoch)\n",
        "\n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_metrics['val_loss']:.4f}\")\n",
        "        print(f\"Train Similarity: {train_similarity:.4f}\")\n",
        "        print(f\"Val Similarity: {val_metrics['val_similarity']:.4f}\")\n",
        "        for k in [1, 5, 10]:\n",
        "            print(f\"Train R@{k}: {train_recalls[k]:.4f}\")\n",
        "            print(f\"Val R@{k}: {val_metrics[f'image_to_text_recall@{k}']:.4f}\")\n",
        "            print(f\"Val P@{k}: {val_metrics[f'image_to_text_precision@{k}']:.4f}\")\n",
        "\n",
        "        # Save best model with memory optimization\n",
        "        current_recall = val_metrics['image_to_text_recall@5']\n",
        "        if val_metrics['val_loss'] < best_val_loss or current_recall > best_recall:\n",
        "            best_val_loss = min(val_metrics['val_loss'], best_val_loss)\n",
        "            best_recall = max(current_recall, best_recall)\n",
        "\n",
        "            torch.cuda.empty_cache()  # Clear cache before saving\n",
        "\n",
        "            save_dict = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'best_recall': best_recall,\n",
        "                'metrics_history': metrics_history\n",
        "            }\n",
        "\n",
        "            torch.save(save_dict, f'best_model_epoch_{epoch+1}.pth')\n",
        "            del save_dict  # Free memory\n",
        "\n",
        "            print(f\"Saved best model - Val Loss: {best_val_loss:.4f}, R@5: {best_recall:.4f}\")\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "            if early_stopping_counter >= CFG.patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        # Memory cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "tkgMKcKUQCXS",
        "outputId": "4303706c-a6fa-405d-c2a2-b902ad2e8976"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting optimized training...\n",
            "Loaded 88641 valid caption entries\n",
            "Found 70919 valid images out of 70919\n",
            "Found 17722 valid images out of 17722\n",
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/4433 [00:03<4:31:39,  3.68s/it, loss=0.1929, sim=0.3906, recall_5=0.0000, lr=0.000000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Attempting to unscale FP16 gradients.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6c8afb15b6cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-6c8afb15b6cb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# Training with recall tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         train_loss, train_similarity, train_recalls = train(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-28-6c8afb15b6cb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, scaler, epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    258\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the Google Drive path for saving the model\n",
        "save_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombinedfinalnowok22pro.pt\"\n",
        "\n",
        "# Define your model class\n",
        "model = CLIPModel().to(CFG.device)\n",
        "\n",
        "# Load the checkpoint\n",
        "checkpoint_path = \"/content/best_model_epoch_20.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, weights_only=True)  # Add weights_only=True to address the warning\n",
        "\n",
        "# Extract just the model state dict from the checkpoint\n",
        "if \"model_state_dict\" in checkpoint:\n",
        "    # If the checkpoint contains the full training state\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "else:\n",
        "    # If the checkpoint contains only the model state\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "# Save the model's state dictionary to the specified Google Drive path\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully at: {save_path}\")"
      ],
      "metadata": {
        "id": "UjwA--gvVzF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJxU0NebXSLj"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_image_embeddings(dataframe, model_path):\n",
        "    \"\"\"\n",
        "    Loads a model and generates image embeddings for the provided dataset.\n",
        "    Parameters:\n",
        "    - dataframe: DataFrame containing image file names.\n",
        "    - model_path: Path to the saved model file.\n",
        "    Returns:\n",
        "    - model: Loaded CLIPModel instance.\n",
        "    - image_embeddings: Tensor of image embeddings for the dataset.\n",
        "    \"\"\"\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "\n",
        "    # Load model with weights_only=True\n",
        "    checkpoint = torch.load(model_path, map_location=CFG.device, weights_only=True)\n",
        "    if \"model_state_dict\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data loader\n",
        "    transforms = get_transforms(mode=\"valid\")\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Generate image embeddings\n",
        "    image_embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Generating image embeddings\"):\n",
        "            images = batch[\"image\"].to(CFG.device)\n",
        "            image_features = model.image_encoder(images)\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            image_embeddings_list.append(image_embeddings)\n",
        "\n",
        "            # Free up memory\n",
        "            if len(image_embeddings_list) % 10 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "    return model, image_embeddings\n",
        "\n",
        "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
        "    \"\"\"\n",
        "    Find matching images for a given text query.\n",
        "    Parameters:\n",
        "    - model: Loaded CLIPModel instance\n",
        "    - image_embeddings: Pre-computed image embeddings\n",
        "    - query: Text query (in Bangla)\n",
        "    - image_filenames: List of image filenames\n",
        "    - n: Number of matches to return\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    # Tokenize the query\n",
        "    inputs = tokenizer(\n",
        "        query,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=CFG.max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move inputs to device\n",
        "    inputs = {k: v.to(CFG.device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get text features\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "        # Calculate similarities\n",
        "        similarities = torch.cosine_similarity(\n",
        "            text_embeddings.unsqueeze(1),\n",
        "            image_embeddings.unsqueeze(0),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # Get top matches\n",
        "        best_matches = torch.topk(similarities[0], k=n)\n",
        "        match_indices = best_matches.indices.cpu().numpy()\n",
        "        match_scores = best_matches.values.cpu().numpy()\n",
        "\n",
        "    return [(image_filenames[idx], score) for idx, score in zip(match_indices, match_scores)]\n",
        "\n",
        "def main():\n",
        "    # Load validation data\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    print(f\"Loaded {len(valid_df)} valid caption entries\")\n",
        "\n",
        "    # Set the correct model path\n",
        "    model_path = \"/content/best_model_epoch_20.pth\"\n",
        "\n",
        "    try:\n",
        "        # Generate the model and image embeddings\n",
        "        model, image_embeddings = get_image_embeddings(valid_df, model_path)\n",
        "\n",
        "        # Define multiple Bangla queries\n",
        "        queries = [\n",
        "            \"  \",  # Cox's Bazar sea beach\n",
        "            \"  \",   # Historical architecture of Dhaka\n",
        "            \"   \",  # Royal Bengal Tiger of Sundarbans\n",
        "            \"   \",  # Natural scenery of Chittagong Hill Tracts\n",
        "            \"  \"  # Shahjalal International Airport\n",
        "        ]\n",
        "\n",
        "        # Process each query and display results\n",
        "        for query in queries:\n",
        "            print(f\"\\n{'='*50}\")\n",
        "            print(f\"Query: {query}\")\n",
        "            print(f\"{'='*50}\")\n",
        "\n",
        "            matches = find_matches(\n",
        "                model=model,\n",
        "                image_embeddings=image_embeddings,\n",
        "                query=query,\n",
        "                image_filenames=valid_df[\"image\"].values,\n",
        "                n=5  # Showing top 5 matches for each query\n",
        "            )\n",
        "\n",
        "            # Print results with detailed similarity scores\n",
        "            print(\"\\nTop matches:\")\n",
        "            print(f\"{'Rank':<6}{'Similarity':<12}{'Image Path'}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "            for idx, (image_path, score) in enumerate(matches, 1):\n",
        "                similarity_percentage = score * 100\n",
        "                print(f\"{idx:<6}{similarity_percentage:.2f}%{' '*4}{image_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "K-DWd0QxYxUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Install required packages\n",
        "!pip install huggingface-hub transformers\n",
        "\n",
        "# Cell 2: Import necessary libraries\n",
        "from huggingface_hub import HfApi, login\n",
        "from transformers import AutoProcessor, CLIPProcessor\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Cell 3: Create configuration file\n",
        "config = {\n",
        "    \"image_size\": CFG.size,\n",
        "    \"projection_dim\": CFG.projection_dim,\n",
        "    \"temperature\": CFG.temperature,\n",
        "    \"image_encoder\": CFG.model_name,\n",
        "    \"text_encoder\": CFG.text_encoder_model,\n",
        "    \"max_length\": CFG.max_length,\n",
        "    \"image_embedding\": CFG.image_embedding,\n",
        "    \"text_embedding\": CFG.text_embedding\n",
        "}\n",
        "with open(\"config.json\", \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "\n",
        "# Cell 4: Load and prepare model with error handling\n",
        "try:\n",
        "    model = CLIPModel().to('cpu')\n",
        "    checkpoint = torch.load(\"/content/best_model_epoch_20.pth\",  # Updated path to match your model\n",
        "                          map_location='cpu',\n",
        "                          weights_only=True)  # Added weights_only parameter\n",
        "\n",
        "    if \"model_state_dict\" in checkpoint:\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"Model loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {str(e)}\")\n",
        "\n",
        "# Cell 5: Create directories and save model components with error handling\n",
        "try:\n",
        "    model_dir = \"BanglaCLIP14\"  # Updated version number\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "    # Save model components\n",
        "    torch.save(model.image_encoder.state_dict(), f\"{model_dir}/image_encoder.pt\")\n",
        "    torch.save(model.text_encoder.state_dict(), f\"{model_dir}/text_encoder.pt\")\n",
        "    torch.save(model.image_projection.state_dict(), f\"{model_dir}/image_projection.pt\")\n",
        "    torch.save(model.text_projection.state_dict(), f\"{model_dir}/text_projection.pt\")\n",
        "\n",
        "    # Also save config\n",
        "    with open(f\"{model_dir}/config.json\", \"w\") as f:\n",
        "        json.dump(config, f, indent=2)\n",
        "\n",
        "    print(\"Model components saved successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model components: {str(e)}\")\n",
        "\n",
        "# Cell 6: Create model card\n",
        "model_card = f\"\"\"# BanglaCLIP14\n",
        "\n",
        "This is a CLIP model trained on Bengali (Bangla) text and images. The model can be used for:\n",
        "- Image-text similarity matching\n",
        "- Zero-shot image classification\n",
        "- Text-to-image retrieval\n",
        "- Image-to-text retrieval\n",
        "\n",
        "## Model Details\n",
        "- Trained on combined dataset of Flickr8k-Bengali, BNATURE, and Bangla Lekha\n",
        "- Image Encoder: ResNet50\n",
        "- Text Encoder: BanglaBERT\n",
        "- Projection Dimension: {CFG.projection_dim}\n",
        "- Training Temperature: {CFG.temperature}\n",
        "\n",
        "## Usage\n",
        "```python\n",
        "from transformers import AutoProcessor, CLIPModel\n",
        "import torch\n",
        "\n",
        "# Load model\n",
        "model = CLIPModel.from_pretrained(\"Mansuba/BanglaCLIP14\")\n",
        "processor = AutoProcessor.from_pretrained(\"Mansuba/BanglaCLIP14\")\n",
        "\n",
        "# Example usage code will go here"
      ],
      "metadata": {
        "id": "4OJJr4_EZL9h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}