{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WUj_UmXU-z0T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining"
      ],
      "metadata": {
        "id": "2Fe9Bx7wOcHJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFEBx9v_wC-",
        "outputId": "120c1b95-316a-46da-dd60-65cdd0ddb417"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add these at the start of your script\n",
        "import torch\n",
        "\n",
        "# Enable cuDNN benchmarking and TF32 for better performance\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True"
      ],
      "metadata": {
        "id": "ujO80AxR7vRq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add get_lr function\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.get_param_groups():\n",
        "        return param_group['lr']\n",
        "\n",
        "class CFG:\n",
        "    debug = False\n",
        "    # Dataset paths\n",
        "    image_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "    captions_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset'\n",
        "    image_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "    captions_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n",
        "    image_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/images'\n",
        "    captions_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/captions.json'\n",
        "\n",
        "    # Training parameters\n",
        "    debug = False\n",
        "    batch_size = 64\n",
        "    num_workers = 4\n",
        "    head_lr = 1e-3\n",
        "    prefetch_factor = 4\n",
        "    pin_memory = True\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-3\n",
        "    patience = 1\n",
        "    factor = 0.8\n",
        "    epochs = 10\n",
        "    gradient_accumulation_steps = 4\n",
        "\n",
        "    # Add caching configurations\n",
        "    use_cached_features = True\n",
        "    cache_dir = './cache'\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 200\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 1.0\n",
        "    size = 224\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1"
      ],
      "metadata": {
        "id": "yanEEq4l7eK1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions():\n",
        "    \"\"\"Load captions with improved validation\"\"\"\n",
        "    captions_list = []\n",
        "\n",
        "    # Load Flickr8k dataset\n",
        "    try:\n",
        "        with open(os.path.join(CFG.captions_path1, 'BAN-Cap_captiondata.json'), 'r', encoding='utf-8') as f:\n",
        "            captions_data1 = json.load(f)\n",
        "\n",
        "        for entry in captions_data1:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id']).split('#')[0]\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Flickr8k dataset: {str(e)}\")\n",
        "\n",
        "    # Load BNATURE dataset\n",
        "    try:\n",
        "        with open(CFG.captions_path2, 'r', encoding='utf-8') as f:\n",
        "            captions_data2 = json.load(f)\n",
        "\n",
        "        for entry in captions_data2:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id'])\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BNATURE dataset: {str(e)}\")\n",
        "\n",
        "    # Load Bangla Lekha dataset with improved handling\n",
        "    try:\n",
        "        with open(CFG.captions_path3, 'r', encoding='utf-8') as f:\n",
        "            captions_data3 = json.load(f)\n",
        "\n",
        "        if isinstance(captions_data3, list):\n",
        "            for entry in captions_data3:\n",
        "                if isinstance(entry, dict) and 'filename' in entry and 'caption' in entry:\n",
        "                    filename = str(entry['filename'])\n",
        "                    caption = str(entry['caption'])\n",
        "                    if caption and filename:\n",
        "                        captions_list.append({\n",
        "                            \"image\": filename.strip(),\n",
        "                            \"caption\": caption.strip()\n",
        "                        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Bangla Lekha dataset: {str(e)}\")\n",
        "\n",
        "    df = pd.DataFrame(captions_list)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "    df['id'] = df.index // 5\n",
        "\n",
        "    print(f\"Loaded {len(df)} valid caption entries\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "pbY8J5qT7vjL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified CLIPDataset class with memory optimizations\n",
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        # Don't pre-encode all captions - do it on-the-fly instead\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Only store valid indices to save memory\n",
        "        self.valid_indices = []\n",
        "        for idx in range(len(self.image_filenames)):\n",
        "            try:\n",
        "                image_found = False\n",
        "                for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                    if os.path.exists(os.path.join(path, self.image_filenames[idx])):\n",
        "                        image_found = True\n",
        "                        break\n",
        "                if image_found:\n",
        "                    self.valid_indices.append(idx)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} valid images out of {len(image_filenames)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            actual_idx = self.valid_indices[idx]\n",
        "\n",
        "            # Encode captions on-the-fly\n",
        "            caption = self.captions[actual_idx]\n",
        "            encoded_caption = self.tokenizer(\n",
        "                caption,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=CFG.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            item = {\n",
        "                'input_ids': encoded_caption['input_ids'][0],\n",
        "                'attention_mask': encoded_caption['attention_mask'][0],\n",
        "            }\n",
        "\n",
        "            image_path = None\n",
        "            for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                if os.path.exists(os.path.join(path, self.image_filenames[actual_idx])):\n",
        "                    image_path = path\n",
        "                    break\n",
        "\n",
        "            if image_path is None:\n",
        "                raise FileNotFoundError(f\"Image {self.image_filenames[actual_idx]} not found in any path\")\n",
        "\n",
        "            # Read image and convert to RGB\n",
        "            image = cv2.imread(os.path.join(image_path, self.image_filenames[actual_idx]))\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load image: {self.image_filenames[actual_idx]}\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = self.transforms(image=image)['image']\n",
        "            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "            item['caption'] = caption\n",
        "\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n"
      ],
      "metadata": {
        "id": "vKS0bO2B9fQu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    \"\"\"\n",
        "    Build data loaders with error handling\n",
        "    \"\"\"\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    try:\n",
        "        dataset = CLIPDataset(\n",
        "            image_filenames=dataframe[\"image\"].values,\n",
        "            captions=dataframe[\"caption\"].values,\n",
        "            tokenizer=tokenizer,\n",
        "            transforms=transforms\n",
        "        )\n",
        "\n",
        "        # Custom collate function to handle potential None values\n",
        "        def collate_fn(batch):\n",
        "            # Filter out None values\n",
        "            batch = [item for item in batch if item is not None]\n",
        "            if len(batch) == 0:\n",
        "                raise RuntimeError(\"Empty batch after filtering\")\n",
        "\n",
        "            return {\n",
        "                'image': torch.stack([item['image'] for item in batch]),\n",
        "                'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "                'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "                'caption': [item['caption'] for item in batch]\n",
        "            }\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=CFG.batch_size,\n",
        "            num_workers=CFG.num_workers,\n",
        "            shuffle=True if mode == \"train\" else False,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=True  # Drop incomplete batches\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building dataloader: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "nWuZvOjiuPS4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iG5GLPfx-0f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = AutoModel(config=AutoConfig.from_pretrained(model_name))\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cZbiFytl-0ie"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YbMomVsf-0lB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HModcbvZ-0ne"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "FSqff5xX-0qA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_train_valid_dfs():\n",
        "    dataframe = load_captions()\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data prefetcher for faster data loading\n",
        "class DataPrefetcher:\n",
        "    def __init__(self, loader, device):\n",
        "        self.loader = iter(loader)\n",
        "        self.device = device\n",
        "        self.stream = torch.cuda.Stream()\n",
        "        self.batch = None\n",
        "        self.preload()\n",
        "\n",
        "    def preload(self):\n",
        "        try:\n",
        "            self.batch = next(self.loader)\n",
        "        except StopIteration:\n",
        "            self.batch = None\n",
        "            return\n",
        "\n",
        "        with torch.cuda.stream(self.stream):\n",
        "            for k, v in self.batch.items():\n",
        "                if k != \"caption\" and isinstance(v, torch.Tensor):\n",
        "                    self.batch[k] = v.to(self.device, non_blocking=True)\n",
        "\n",
        "    def next(self):\n",
        "        torch.cuda.current_stream().wait_stream(self.stream)\n",
        "        batch = self.batch\n",
        "        self.preload()\n",
        "        return batch"
      ],
      "metadata": {
        "id": "FgIQ8N34u1T_"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "OGhQqWJeHfVn"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Compute cosine similarity\n",
        "def compute_cosine_similarity(embeddings_a, embeddings_b):\n",
        "    a = torch.nn.functional.normalize(embeddings_a, p=2, dim=-1)\n",
        "    b = torch.nn.functional.normalize(embeddings_b, p=2, dim=-1)\n",
        "    return torch.mm(a, b.t())\n",
        "\n",
        "# Precision and Recall computation\n",
        "def precision_recall(similarity_matrix, k=5):\n",
        "    num_samples = similarity_matrix.size(0)\n",
        "    relevant_items = torch.eye(num_samples, device=similarity_matrix.device)\n",
        "    top_k_indices = similarity_matrix.topk(k, dim=-1).indices\n",
        "\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    for i in range(num_samples):\n",
        "        matches = relevant_items[i][top_k_indices[i]].sum().item()\n",
        "        precision += matches / k\n",
        "        recall += matches / relevant_items[i].sum().item()\n",
        "\n",
        "    precision /= num_samples\n",
        "    recall /= num_samples\n",
        "    return precision, recall\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    total_precision = 0\n",
        "    total_recall = 0\n",
        "    num_chunks = 0\n",
        "\n",
        "    # Enable automatic mixed precision\n",
        "    scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "    # Cache for storing computed features\n",
        "    if not hasattr(model, 'feature_cache'):\n",
        "        model.feature_cache = {}\n",
        "\n",
        "    # Pre-fetch data to GPU\n",
        "    prefetcher = DataPrefetcher(train_loader, CFG.device)\n",
        "    batch = prefetcher.next()\n",
        "\n",
        "    batch_idx = 0\n",
        "    pbar = tqdm(total=len(train_loader), colour=\"green\")\n",
        "\n",
        "    while batch is not None:\n",
        "        batch_key = hash(str(batch['input_ids'].cpu().numpy().tobytes()))\n",
        "\n",
        "        # Use cached features if available (after first epoch)\n",
        "        if batch_key in model.feature_cache:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                cached_data = model.feature_cache[batch_key]\n",
        "                image_features = cached_data['image_features']\n",
        "                text_features = cached_data['text_features']\n",
        "\n",
        "                # Only compute the projections and loss\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                # Calculate loss using pre-computed features\n",
        "                logits = (text_embeddings @ image_embeddings.T) / model.temperature\n",
        "                images_similarity = image_embeddings @ image_embeddings.T\n",
        "                texts_similarity = text_embeddings @ text_embeddings.T\n",
        "                targets = F.softmax(\n",
        "                    (images_similarity + texts_similarity) / 2 * model.temperature, dim=-1\n",
        "                )\n",
        "                texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "                images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "                loss = (images_loss + texts_loss) / 2.0\n",
        "                loss = loss.mean()\n",
        "        else:\n",
        "            # First epoch: compute and cache features\n",
        "            with torch.cuda.amp.autocast():\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "\n",
        "                # Cache the computed features\n",
        "                model.feature_cache[batch_key] = {\n",
        "                    'image_features': image_features.detach(),\n",
        "                    'text_features': text_features.detach()\n",
        "                }\n",
        "\n",
        "                # Complete forward pass\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                logits = (text_embeddings @ image_embeddings.T) / model.temperature\n",
        "                images_similarity = image_embeddings @ image_embeddings.T\n",
        "                texts_similarity = text_embeddings @ text_embeddings.T\n",
        "                targets = F.softmax(\n",
        "                    (images_similarity + texts_similarity) / 2 * model.temperature, dim=-1\n",
        "                )\n",
        "                texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "                images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "                loss = (images_loss + texts_loss) / 2.0\n",
        "                loss = loss.mean()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Gradient accumulation and optimization\n",
        "        if (batch_idx + 1) % 4 == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "            if step == \"batch\":\n",
        "                lr_scheduler.step()\n",
        "\n",
        "        # Calculate metrics less frequently (every 50 batches)\n",
        "        if num_chunks % 50 == 0:\n",
        "            with torch.no_grad():\n",
        "                similarity_matrix = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "                precision, recall = precision_recall(similarity_matrix, k=5)\n",
        "                total_precision += precision\n",
        "                total_recall += recall\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        # Update progress bar\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        pbar.set_postfix(\n",
        "            train_loss=loss_meter.avg,\n",
        "            lr=current_lr,\n",
        "            avg_precision=total_precision/max(1, num_chunks//50),\n",
        "            avg_recall=total_recall/max(1, num_chunks//50)\n",
        "        )\n",
        "        pbar.update(1)\n",
        "\n",
        "        # Get next batch\n",
        "        batch = prefetcher.next()\n",
        "        batch_idx += 1\n",
        "        num_chunks += 1\n",
        "\n",
        "    pbar.close()\n",
        "    return loss_meter, total_precision/max(1, num_chunks//50), total_recall/max(1, num_chunks//50)\n",
        "\n",
        "def main():\n",
        "    print(\"Starting training...\")\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay},\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
        "    )\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_precisions = []\n",
        "    train_recalls = []\n",
        "    val_precisions = []\n",
        "    val_recalls = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"\\nStarting Epoch {epoch + 1}...\")\n",
        "\n",
        "        # Training\n",
        "        train_loss, train_precision, train_recall = train(model, train_loader, optimizer, lr_scheduler, step=\"epoch\")\n",
        "        train_losses.append(train_loss.avg)\n",
        "        train_precisions.append(train_precision)\n",
        "        train_recalls.append(train_recall)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss, val_precision, val_recall = validate(model, valid_loader)\n",
        "        val_losses.append(valid_loss.avg)\n",
        "        val_precisions.append(val_precision)\n",
        "        val_recalls.append(val_recall)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{CFG.epochs}\")\n",
        "        print(f\"Training Loss: {train_loss.avg:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "        print(f\"Validation Loss: {valid_loss.avg:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
        "\n",
        "        lr_scheduler.step(valid_loss.avg)\n",
        "\n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\"Saved best model with validation loss: {best_loss:.4f}\")\n",
        "\n",
        "        # Plot Loss and Metrics\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot Training and Validation Loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        epochs_range = range(1, epoch + 2)\n",
        "        plt.plot(epochs_range, train_losses, label=\"Train Loss\", marker=\"o\", color=\"blue\")\n",
        "        plt.plot(epochs_range, val_losses, label=\"Val Loss\", marker=\"x\", color=\"green\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Training Metrics\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs_range, train_precisions, label=\"Precision@5\", marker=\"o\", color=\"orange\")\n",
        "        plt.plot(epochs_range, train_recalls, label=\"Recall@5\", marker=\"x\", color=\"purple\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Metrics\")\n",
        "        plt.title(\"Training Precision and Recall\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Validation Metrics\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs_range, val_precisions, label=\"Precision@5\", marker=\"o\", color=\"orange\")\n",
        "        plt.plot(epochs_range, val_recalls, label=\"Recall@5\", marker=\"x\", color=\"purple\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Metrics\")\n",
        "        plt.title(\"Validation Precision and Recall\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "-xlATwXJ8Vav",
        "outputId": "1643d982-7d76-4182-b8f2-2dc196d27af2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Loaded 88641 valid caption entries\n",
            "Found 70919 valid images out of 70919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17722 valid images out of 17722\n",
            "\n",
            "Starting Epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|\u001b[32m          \u001b[0m| 0/1108 [00:00<?, ?it/s]\u001b[A<ipython-input-45-a51486b494fb>:81: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 15.06 MiB is free. Process 8894 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 462.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-a51486b494fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-a51486b494fb>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mtrain_precisions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-a51486b494fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, step)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# First epoch: compute and cache features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 text_features = model.text_encoder(\n\u001b[1;32m     84\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-d3165b27e2a1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 15.06 MiB is free. Process 8894 has 14.73 GiB memory in use. Of the allocated memory 14.11 GiB is allocated by PyTorch, and 462.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the Google Drive path for saving the model\n",
        "save_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombinedfinalnow.pt\"\n",
        "\n",
        "# Define your model class (ensure this matches the architecture used during training)\n",
        "model = CLIPModel().to(CFG.device)  # Replace CLIPModel with your actual model class\n",
        "\n",
        "# Load the trained model from the saved state dictionary\n",
        "checkpoint_path = \"/content/best_model.pth\"\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# Save the model's state dictionary to the specified Google Drive path\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully at: {save_path}\")\n"
      ],
      "metadata": {
        "id": "JvZ37sXX3grB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJxU0NebXSLj"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(dataframe, model_path):\n",
        "    \"\"\"\n",
        "    Loads a model and generates image embeddings for the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataframe: DataFrame containing image file names.\n",
        "    - model_path: Path to the saved model file.\n",
        "\n",
        "    Returns:\n",
        "    - model: Loaded CLIPModel instance.\n",
        "    - image_embeddings: Tensor of image embeddings for the dataset.\n",
        "    \"\"\"\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data loader\n",
        "    transforms = get_transforms(mode=\"valid\")\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Generate image embeddings\n",
        "    image_embeddings_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, colour=\"yellow\", desc=\"Generating image embeddings\"):\n",
        "            images = batch[\"image\"].to(CFG.device)\n",
        "            image_features = model.image_encoder(images)\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            image_embeddings_list.append(image_embeddings)\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "    return model, image_embeddings\n",
        "\n",
        "\n",
        "# Interface Function to Use find_matches\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the validation DataFrame and model are ready\n",
        "    _, valid_df = make_train_valid_dfs()\n",
        "\n",
        "    # Generate the model and image embeddings\n",
        "    model, image_embeddings = get_image_embeddings(valid_df, \"best_model.pth\")\n",
        "\n",
        "    # Define the Bangla text query and display matches\n",
        "    query = \"  \"  # Bangla query for \"cycle\"\n",
        "    find_matches(\n",
        "        model=model,\n",
        "        image_embeddings=image_embeddings,\n",
        "        query=query,\n",
        "        image_filenames=valid_df[\"image\"].values,\n",
        "        n=9,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "smKkrI30-kr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDIebVgkCD-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gbHD-E7CDnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OXi50mMqCDd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}