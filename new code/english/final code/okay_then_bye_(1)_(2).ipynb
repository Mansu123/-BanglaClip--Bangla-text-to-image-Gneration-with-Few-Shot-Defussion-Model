{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WUj_UmXU-z0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cee3cce-ce4e-4501-ca1c-fb29e07a6850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-aaa55f4caea2>:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n",
            "/usr/local/lib/python3.11/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.0 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining"
      ],
      "metadata": {
        "id": "2Fe9Bx7wOcHJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFEBx9v_wC-",
        "outputId": "d8083161-085e-4972-d9fe-944a5a4d181a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add get_lr function\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "class CFG:\n",
        "    debug = False\n",
        "    # Dataset paths\n",
        "    image_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "    captions_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset'\n",
        "    image_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "    captions_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n",
        "    image_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/images'\n",
        "    captions_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/captions.json'\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 32  # Reduced batch size\n",
        "    gradient_accumulation_steps = 4  # Increased accumulation steps\n",
        "    num_workers = 2\n",
        "    pin_memory = True\n",
        "    # Mixed precision settings\n",
        "    mixed_precision = True\n",
        "\n",
        "    # Model settings\n",
        "\n",
        "    attention_dropout_prob = 0.1\n",
        "\n",
        "    # Optimizer settings\n",
        "    weight_decay = 0.01\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    # Learning rates\n",
        "    image_encoder_lr = 1e-5\n",
        "    text_encoder_lr = 1e-5\n",
        "    head_lr = 1e-4\n",
        "    prefetch_factor = 2\n",
        "    patience = 1\n",
        "    hidden_dropout_prob=0.1\n",
        "    factor = 0.8\n",
        "    epochs = 2\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 200\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 1.0\n",
        "    size = 224\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1"
      ],
      "metadata": {
        "id": "alQ8syEyRrFF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions():\n",
        "    \"\"\"Load captions with improved validation\"\"\"\n",
        "    captions_list = []\n",
        "\n",
        "    # Load Flickr8k dataset\n",
        "    try:\n",
        "        with open(os.path.join(CFG.captions_path1, 'BAN-Cap_captiondata.json'), 'r', encoding='utf-8') as f:\n",
        "            captions_data1 = json.load(f)\n",
        "\n",
        "        for entry in captions_data1:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id']).split('#')[0]\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Flickr8k dataset: {str(e)}\")\n",
        "\n",
        "    # Load BNATURE dataset\n",
        "    try:\n",
        "        with open(CFG.captions_path2, 'r', encoding='utf-8') as f:\n",
        "            captions_data2 = json.load(f)\n",
        "\n",
        "        for entry in captions_data2:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id'])\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BNATURE dataset: {str(e)}\")\n",
        "\n",
        "    # Load Bangla Lekha dataset with improved handling\n",
        "    try:\n",
        "        with open(CFG.captions_path3, 'r', encoding='utf-8') as f:\n",
        "            captions_data3 = json.load(f)\n",
        "\n",
        "        if isinstance(captions_data3, list):\n",
        "            for entry in captions_data3:\n",
        "                if isinstance(entry, dict) and 'filename' in entry and 'caption' in entry:\n",
        "                    filename = str(entry['filename'])\n",
        "                    caption = str(entry['caption'])\n",
        "                    if caption and filename:\n",
        "                        captions_list.append({\n",
        "                            \"image\": filename.strip(),\n",
        "                            \"caption\": caption.strip()\n",
        "                        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Bangla Lekha dataset: {str(e)}\")\n",
        "\n",
        "    df = pd.DataFrame(captions_list)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "    df['id'] = df.index // 5\n",
        "\n",
        "    print(f\"Loaded {len(df)} valid caption entries\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "pbY8J5qT7vjL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions),\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.valid_indices = []\n",
        "        for idx in range(len(self.image_filenames)):\n",
        "            try:\n",
        "                image_found = False\n",
        "                for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                    if os.path.exists(os.path.join(path, self.image_filenames[idx])):\n",
        "                        image_found = True\n",
        "                        break\n",
        "\n",
        "                if image_found:\n",
        "                    self.valid_indices.append(idx)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} valid images out of {len(image_filenames)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            actual_idx = self.valid_indices[idx]\n",
        "\n",
        "            item = {\n",
        "                'input_ids': self.encoded_captions['input_ids'][actual_idx],\n",
        "                'attention_mask': self.encoded_captions['attention_mask'][actual_idx],\n",
        "            }\n",
        "\n",
        "            image_path = None\n",
        "            for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                if os.path.exists(os.path.join(path, self.image_filenames[actual_idx])):\n",
        "                    image_path = path\n",
        "                    break\n",
        "\n",
        "            if image_path is None:\n",
        "                raise FileNotFoundError(f\"Image {self.image_filenames[actual_idx]} not found in any path\")\n",
        "\n",
        "            image = cv2.imread(os.path.join(image_path, self.image_filenames[actual_idx]))\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load image: {self.image_filenames[actual_idx]}\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = self.transforms(image=image)['image']\n",
        "            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "            item['caption'] = self.captions[actual_idx]\n",
        "\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)"
      ],
      "metadata": {
        "id": "Ux2LXdjv8W1O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    \"\"\"\n",
        "    Build data loaders with error handling\n",
        "    \"\"\"\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    try:\n",
        "        dataset = CLIPDataset(\n",
        "            image_filenames=dataframe[\"image\"].values,\n",
        "            captions=dataframe[\"caption\"].values,\n",
        "            tokenizer=tokenizer,\n",
        "            transforms=transforms\n",
        "        )\n",
        "\n",
        "        # Custom collate function to handle potential None values\n",
        "        def collate_fn(batch):\n",
        "            # Filter out None values\n",
        "            batch = [item for item in batch if item is not None]\n",
        "            if len(batch) == 0:\n",
        "                raise RuntimeError(\"Empty batch after filtering\")\n",
        "\n",
        "            return {\n",
        "                'image': torch.stack([item['image'] for item in batch]),\n",
        "                'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "                'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "                'caption': [item['caption'] for item in batch]\n",
        "            }\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=CFG.batch_size,\n",
        "            num_workers=CFG.num_workers,\n",
        "            shuffle=True if mode == \"train\" else False,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=True  # Drop incomplete batches\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building dataloader: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "nWuZvOjiuPS4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iG5GLPfx-0f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = AutoModel(config=AutoConfig.from_pretrained(model_name))\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = F.cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "jtKuygbHRxip"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YbMomVsf-0lB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HModcbvZ-0ne"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FSqff5xX-0qA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_train_valid_dfs():\n",
        "    dataframe = load_captions()\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A-D3J1fG-0sn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "OGhQqWJeHfVn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "clFv516SDOoY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "# Memory optimizations\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "# Configure PyTorch memory allocator\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128,garbage_collection_threshold:0.8,expandable_segments:True'\n"
      ],
      "metadata": {
        "id": "HXL2PKR3M5xD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set memory and speed optimizations\n",
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "\n",
        "# Configure PyTorch memory allocator\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512,expandable_segments:True'\n",
        "\n",
        "def compute_cosine_similarity(embeddings_a, embeddings_b, temperature=0.07):\n",
        "    # Process in chunks to save memory\n",
        "    chunk_size = 256\n",
        "    num_chunks = (embeddings_a.size(0) + chunk_size - 1) // chunk_size\n",
        "    similarity_chunks = []\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = min((i + 1) * chunk_size, embeddings_a.size(0))\n",
        "        chunk_a = embeddings_a[start_idx:end_idx]\n",
        "\n",
        "        # Normalize chunks\n",
        "        chunk_a = torch.nn.functional.normalize(chunk_a, p=2, dim=-1)\n",
        "        chunk_b = torch.nn.functional.normalize(embeddings_b, p=2, dim=-1)\n",
        "\n",
        "        # Compute similarity for chunk\n",
        "        chunk_sim = torch.mm(chunk_a, chunk_b.t()) / temperature\n",
        "        similarity_chunks.append(chunk_sim)\n",
        "\n",
        "    return torch.cat(similarity_chunks, dim=0)\n",
        "\n",
        "def precision_recall(similarity_matrix, k=5):\n",
        "    num_samples = similarity_matrix.size(0)\n",
        "    relevant_items = torch.eye(num_samples, device=similarity_matrix.device)\n",
        "    top_k_indices = similarity_matrix.topk(k, dim=-1).indices\n",
        "\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    for i in range(num_samples):\n",
        "        matches = relevant_items[i][top_k_indices[i]].sum().item()\n",
        "        precision += matches / k\n",
        "        recall += matches / relevant_items[i].sum().item()\n",
        "\n",
        "    precision /= num_samples\n",
        "    recall /= num_samples\n",
        "    return precision, recall\n",
        "\n",
        "def train(model, train_loader, optimizer, lr_scheduler, scaler, epoch):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    # Process fewer batches for validation\n",
        "    eval_steps = min(100, len(train_loader))\n",
        "\n",
        "    image_embeddings_list = []\n",
        "    text_embeddings_list = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(tqdm(train_loader, total=len(train_loader))):\n",
        "        # Move to GPU and handle memory\n",
        "        batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        # Clear memory\n",
        "        if batch_idx % 50 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Mixed precision training\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            loss = model(batch)\n",
        "            loss = loss / CFG.gradient_accumulation_steps\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % CFG.gradient_accumulation_steps == 0:\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            optimizer.zero_grad()\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        # Collect embeddings for evaluation\n",
        "        if batch_idx < eval_steps:\n",
        "            with torch.no_grad():\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                image_embeddings_list.append(image_embeddings.detach().cpu())\n",
        "                text_embeddings_list.append(text_embeddings.detach().cpu())\n",
        "\n",
        "        loss_meter.update(loss.item() * CFG.gradient_accumulation_steps, batch[\"image\"].size(0))\n",
        "\n",
        "        # Early stopping for first epoch\n",
        "        if epoch == 0 and batch_idx >= 200:\n",
        "            break\n",
        "\n",
        "    # Calculate metrics\n",
        "    precision, recall = 0, 0\n",
        "    if image_embeddings_list:\n",
        "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "        text_embeddings = torch.cat(text_embeddings_list, dim=0)\n",
        "\n",
        "        similarity_matrix = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "        precision, recall = precision_recall(similarity_matrix, k=5)\n",
        "\n",
        "        del image_embeddings, text_embeddings, similarity_matrix\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return loss_meter, precision, recall\n",
        "\n",
        "def validate(model, valid_loader):\n",
        "    model.eval()\n",
        "    loss_meter = AvgMeter()\n",
        "\n",
        "    # Reduce validation samples\n",
        "    max_val_samples = 1000\n",
        "\n",
        "    image_embeddings_list = []\n",
        "    text_embeddings_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(valid_loader):\n",
        "            if batch_idx >= max_val_samples:\n",
        "                break\n",
        "\n",
        "            batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "            # Process in half precision\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                image_features = model.image_encoder(batch[\"image\"])\n",
        "                text_features = model.text_encoder(\n",
        "                    input_ids=batch[\"input_ids\"],\n",
        "                    attention_mask=batch[\"attention_mask\"]\n",
        "                )\n",
        "                image_embeddings = model.image_projection(image_features)\n",
        "                text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "                similarity = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "                loss = model.compute_loss(similarity)\n",
        "\n",
        "            image_embeddings_list.append(image_embeddings.cpu())\n",
        "            text_embeddings_list.append(text_embeddings.cpu())\n",
        "\n",
        "            loss_meter.update(loss.item(), batch[\"image\"].size(0))\n",
        "\n",
        "            # Clear memory periodically\n",
        "            if batch_idx % 50 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    precision, recall = 0, 0\n",
        "    if image_embeddings_list:\n",
        "        image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "        text_embeddings = torch.cat(text_embeddings_list, dim=0)\n",
        "\n",
        "        similarity_matrix = compute_cosine_similarity(image_embeddings, text_embeddings)\n",
        "        precision, recall = precision_recall(similarity_matrix, k=5)\n",
        "\n",
        "        del image_embeddings, text_embeddings, similarity_matrix\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return loss_meter, precision, recall\n",
        "\n",
        "def main():\n",
        "    print(\"Starting memory-optimized training...\")\n",
        "\n",
        "    # Initialize lists to store metrics for plotting\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_precisions = []\n",
        "    val_precisions = []\n",
        "    train_recalls = []\n",
        "    val_recalls = []\n",
        "\n",
        "    # Load data with reduced batch size\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    # Remove incorrect arguments and use only the ones defined in CLIPModel __init__\n",
        "    model = CLIPModel(\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding\n",
        "    ).to(CFG.device)\n",
        "\n",
        "    # Load checkpoint\n",
        "    checkpoint_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombinedfinal.pt\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=CFG.device)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    del checkpoint\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Optimizer setup with reduced learning rates\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(),\n",
        "            model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr}\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=CFG.weight_decay)\n",
        "    scaler = torch.amp.GradScaler()\n",
        "\n",
        "    # Scheduler with reduced warmup\n",
        "    num_training_steps = len(train_loader) * CFG.epochs\n",
        "    num_warmup_steps = num_training_steps // 20  # 5% warmup\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=num_warmup_steps,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 2\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}\")\n",
        "\n",
        "        train_loss, train_prec, train_rec = train(\n",
        "            model, train_loader, optimizer, scheduler, scaler, epoch\n",
        "        )\n",
        "\n",
        "        val_loss, val_prec, val_rec = validate(model, valid_loader)\n",
        "\n",
        "        # Append metrics for plotting\n",
        "        train_losses.append(train_loss.avg)\n",
        "        val_losses.append(val_loss.avg)\n",
        "        train_precisions.append(train_prec)\n",
        "        val_precisions.append(val_prec)\n",
        "        train_recalls.append(train_rec)\n",
        "        val_recalls.append(val_rec)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss.avg:.4f}, P@5: {train_prec:.4f}, R@5: {train_rec:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss.avg:.4f}, P@5: {val_prec:.4f}, R@5: {val_rec:.4f}\")\n",
        "\n",
        "        # Save best model and handle early stopping\n",
        "        if val_loss.avg < best_val_loss:\n",
        "            best_val_loss = val_loss.avg\n",
        "            torch.save(model.state_dict(), f\"best_model_epoch_{epoch+1}.pth\")\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "            if early_stopping_counter >= patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        # Clear memory between epochs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        # Plot training progress\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        epochs_range = range(1, epoch + 2)\n",
        "\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.plot(epochs_range, train_losses, label=\"Train Loss\", marker=\"o\", color=\"blue\")\n",
        "        plt.plot(epochs_range, val_losses, label=\"Val Loss\", marker=\"x\", color=\"green\")\n",
        "        plt.title(\"Loss\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs_range, train_precisions, label=\"Train\", marker=\"o\", color=\"orange\")\n",
        "        plt.plot(epochs_range, val_precisions, label=\"Val\", marker=\"x\", color=\"red\")\n",
        "        plt.title(\"Precision@5\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs_range, train_recalls, label=\"Train\", marker=\"o\", color=\"purple\")\n",
        "        plt.plot(epochs_range, val_recalls, label=\"Val\", marker=\"x\", color=\"brown\")\n",
        "        plt.title(\"Recall@5\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Clear memory between epochs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "394GK_P4P6hp",
        "outputId": "b03602e7-f298-4fd4-8f2f-1dc297bdd140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting memory-optimized training...\n",
            "Loaded 88641 valid caption entries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 70919 valid images out of 70919\n",
            "Found 17722 valid images out of 17722\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-b14acf9ed9e6>:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=CFG.device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 12/2217 [02:36<7:04:35, 11.55s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the Google Drive path for saving the model\n",
        "save_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombinedfinalnowok.pt\"\n",
        "\n",
        "# Define your model class (ensure this matches the architecture used during training)\n",
        "model = CLIPModel().to(CFG.device)  # Replace CLIPModel with your actual model class\n",
        "\n",
        "# Load the trained model from the saved state dictionary\n",
        "checkpoint_path = \"/content/best_model.pth\"\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# Save the model's state dictionary to the specified Google Drive path\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully at: {save_path}\")\n"
      ],
      "metadata": {
        "id": "JvZ37sXX3grB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJxU0NebXSLj"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(dataframe, model_path):\n",
        "    \"\"\"\n",
        "    Loads a model and generates image embeddings for the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataframe: DataFrame containing image file names.\n",
        "    - model_path: Path to the saved model file.\n",
        "\n",
        "    Returns:\n",
        "    - model: Loaded CLIPModel instance.\n",
        "    - image_embeddings: Tensor of image embeddings for the dataset.\n",
        "    \"\"\"\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data loader\n",
        "    transforms = get_transforms(mode=\"valid\")\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Generate image embeddings\n",
        "    image_embeddings_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, colour=\"yellow\", desc=\"Generating image embeddings\"):\n",
        "            images = batch[\"image\"].to(CFG.device)\n",
        "            image_features = model.image_encoder(images)\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            image_embeddings_list.append(image_embeddings)\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "    return model, image_embeddings\n",
        "\n",
        "\n",
        "# Interface Function to Use find_matches\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the validation DataFrame and model are ready\n",
        "    _, valid_df = make_train_valid_dfs()\n",
        "\n",
        "    # Generate the model and image embeddings\n",
        "    model, image_embeddings = get_image_embeddings(valid_df, \"best_model.pth\")\n",
        "\n",
        "    # Define the Bangla text query and display matches\n",
        "    query = \"কক্সবাজারের সমুদ্র সৈকত\"  # Bangla query for \"cycle\"\n",
        "    find_matches(\n",
        "        model=model,\n",
        "        image_embeddings=image_embeddings,\n",
        "        query=query,\n",
        "        image_filenames=valid_df[\"image\"].values,\n",
        "        n=9,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "smKkrI30-kr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDIebVgkCD-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gbHD-E7CDnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OXi50mMqCDd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}