{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WUj_UmXU-z0T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForPreTraining"
      ],
      "metadata": {
        "id": "2Fe9Bx7wOcHJ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEFEBx9v_wC-",
        "outputId": "efe164c7-b757-479a-a85b-b751f8ac7ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import albumentations as A\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Add get_lr function\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.get_param_groups():\n",
        "        return param_group['lr']\n",
        "\n",
        "class CFG:\n",
        "    debug = False\n",
        "    # Dataset paths\n",
        "    image_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "    captions_path1 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset'\n",
        "    image_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "    captions_path2 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n",
        "    image_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/images'\n",
        "    captions_path3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/captions.json'\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 32\n",
        "    num_workers = 2\n",
        "    head_lr = 1e-3\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-3\n",
        "    patience = 1\n",
        "    factor = 0.8\n",
        "    epochs = 4\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model parameters\n",
        "    model_name = 'resnet50'\n",
        "    image_embedding = 2048\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    text_embedding = 768\n",
        "    text_tokenizer = \"csebuetnlp/banglabert\"\n",
        "    max_length = 200\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    temperature = 1.0\n",
        "    size = 224\n",
        "    num_projection_layers = 1\n",
        "    projection_dim = 256\n",
        "    dropout = 0.1"
      ],
      "metadata": {
        "id": "yanEEq4l7eK1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_captions():\n",
        "    \"\"\"Load captions with improved validation\"\"\"\n",
        "    captions_list = []\n",
        "\n",
        "    # Load Flickr8k dataset\n",
        "    try:\n",
        "        with open(os.path.join(CFG.captions_path1, 'BAN-Cap_captiondata.json'), 'r', encoding='utf-8') as f:\n",
        "            captions_data1 = json.load(f)\n",
        "\n",
        "        for entry in captions_data1:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id']).split('#')[0]\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Flickr8k dataset: {str(e)}\")\n",
        "\n",
        "    # Load BNATURE dataset\n",
        "    try:\n",
        "        with open(CFG.captions_path2, 'r', encoding='utf-8') as f:\n",
        "            captions_data2 = json.load(f)\n",
        "\n",
        "        for entry in captions_data2:\n",
        "            if isinstance(entry, dict) and 'caption_id' in entry and 'bengali_caption' in entry:\n",
        "                filename = str(entry['caption_id'])\n",
        "                caption = str(entry['bengali_caption'])\n",
        "                if caption and filename:\n",
        "                    captions_list.append({\n",
        "                        \"image\": filename.strip(),\n",
        "                        \"caption\": caption.strip()\n",
        "                    })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading BNATURE dataset: {str(e)}\")\n",
        "\n",
        "    # Load Bangla Lekha dataset with improved handling\n",
        "    try:\n",
        "        with open(CFG.captions_path3, 'r', encoding='utf-8') as f:\n",
        "            captions_data3 = json.load(f)\n",
        "\n",
        "        if isinstance(captions_data3, list):\n",
        "            for entry in captions_data3:\n",
        "                if isinstance(entry, dict) and 'filename' in entry and 'caption' in entry:\n",
        "                    filename = str(entry['filename'])\n",
        "                    caption = str(entry['caption'])\n",
        "                    if caption and filename:\n",
        "                        captions_list.append({\n",
        "                            \"image\": filename.strip(),\n",
        "                            \"caption\": caption.strip()\n",
        "                        })\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading Bangla Lekha dataset: {str(e)}\")\n",
        "\n",
        "    df = pd.DataFrame(captions_list)\n",
        "    df = df.dropna()\n",
        "    df = df.drop_duplicates()\n",
        "    df['id'] = df.index // 5\n",
        "\n",
        "    print(f\"Loaded {len(df)} valid caption entries\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "pbY8J5qT7vjL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions),\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=CFG.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "        self.valid_indices = []\n",
        "        for idx in range(len(self.image_filenames)):\n",
        "            try:\n",
        "                image_found = False\n",
        "                for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                    if os.path.exists(os.path.join(path, self.image_filenames[idx])):\n",
        "                        image_found = True\n",
        "                        break\n",
        "\n",
        "                if image_found:\n",
        "                    self.valid_indices.append(idx)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} valid images out of {len(image_filenames)}\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            actual_idx = self.valid_indices[idx]\n",
        "\n",
        "            item = {\n",
        "                'input_ids': self.encoded_captions['input_ids'][actual_idx],\n",
        "                'attention_mask': self.encoded_captions['attention_mask'][actual_idx],\n",
        "            }\n",
        "\n",
        "            image_path = None\n",
        "            for path in [CFG.image_path1, CFG.image_path2, CFG.image_path3]:\n",
        "                if os.path.exists(os.path.join(path, self.image_filenames[actual_idx])):\n",
        "                    image_path = path\n",
        "                    break\n",
        "\n",
        "            if image_path is None:\n",
        "                raise FileNotFoundError(f\"Image {self.image_filenames[actual_idx]} not found in any path\")\n",
        "\n",
        "            image = cv2.imread(os.path.join(image_path, self.image_filenames[actual_idx]))\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Failed to load image: {self.image_filenames[actual_idx]}\")\n",
        "\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image = self.transforms(image=image)['image']\n",
        "            item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "            item['caption'] = self.captions[actual_idx]\n",
        "\n",
        "            return item\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item {idx}: {str(e)}\")\n",
        "            raise e\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)"
      ],
      "metadata": {
        "id": "Ux2LXdjv8W1O"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    \"\"\"\n",
        "    Build data loaders with error handling\n",
        "    \"\"\"\n",
        "    transforms = get_transforms(mode=mode)\n",
        "\n",
        "    try:\n",
        "        dataset = CLIPDataset(\n",
        "            image_filenames=dataframe[\"image\"].values,\n",
        "            captions=dataframe[\"caption\"].values,\n",
        "            tokenizer=tokenizer,\n",
        "            transforms=transforms\n",
        "        )\n",
        "\n",
        "        # Custom collate function to handle potential None values\n",
        "        def collate_fn(batch):\n",
        "            # Filter out None values\n",
        "            batch = [item for item in batch if item is not None]\n",
        "            if len(batch) == 0:\n",
        "                raise RuntimeError(\"Empty batch after filtering\")\n",
        "\n",
        "            return {\n",
        "                'image': torch.stack([item['image'] for item in batch]),\n",
        "                'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "                'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "                'caption': [item['caption'] for item in batch]\n",
        "            }\n",
        "\n",
        "        dataloader = torch.utils.data.DataLoader(\n",
        "            dataset,\n",
        "            batch_size=CFG.batch_size,\n",
        "            num_workers=CFG.num_workers,\n",
        "            shuffle=True if mode == \"train\" else False,\n",
        "            collate_fn=collate_fn,\n",
        "            drop_last=True  # Drop incomplete batches\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error building dataloader: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "nWuZvOjiuPS4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iG5GLPfx-0f0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(\n",
        "            model_name, pretrained, num_classes=0, global_pool=\"avg\"\n",
        "        )\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            self.model = AutoModel(config=AutoConfig.from_pretrained(model_name))\n",
        "\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "cZbiFytl-0ie"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "YbMomVsf-0lB"
      },
      "outputs": [],
      "source": [
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "HModcbvZ-0ne"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "FSqff5xX-0qA"
      },
      "outputs": [],
      "source": [
        "\n",
        "def make_train_valid_dfs():\n",
        "    dataframe = load_captions()\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "    return train_dataframe, valid_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A-D3J1fG-0sn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "metadata": {
        "id": "OGhQqWJeHfVn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Compute cosine similarity\n",
        "def compute_cosine_similarity(embeddings_a, embeddings_b):\n",
        "    a = torch.nn.functional.normalize(embeddings_a, p=2, dim=-1)\n",
        "    b = torch.nn.functional.normalize(embeddings_b, p=2, dim=-1)\n",
        "    return torch.mm(a, b.t())\n",
        "\n",
        "# Precision and Recall computation\n",
        "def precision_recall(similarity_matrix, k=5):\n",
        "    num_samples = similarity_matrix.size(0)\n",
        "    relevant_items = torch.eye(num_samples, device=similarity_matrix.device)\n",
        "    top_k_indices = similarity_matrix.topk(k, dim=-1).indices\n",
        "\n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    for i in range(num_samples):\n",
        "        matches = relevant_items[i][top_k_indices[i]].sum().item()\n",
        "        precision += matches / k\n",
        "        recall += matches / relevant_items[i].sum().item()\n",
        "\n",
        "    precision /= num_samples\n",
        "    recall /= num_samples\n",
        "    return precision, recall\n",
        "\n",
        "def train(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    model.train()\n",
        "    loss_meter = AvgMeter()\n",
        "    all_image_embeddings = []\n",
        "    all_text_embeddings = []\n",
        "\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader), colour=\"green\")\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model(batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        # Get embeddings for metrics\n",
        "        with torch.no_grad():\n",
        "            image_features = model.image_encoder(batch[\"image\"])\n",
        "            text_features = model.text_encoder(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"]\n",
        "            )\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "            all_image_embeddings.append(image_embeddings.detach().cpu())\n",
        "            all_text_embeddings.append(text_embeddings.detach().cpu())\n",
        "\n",
        "        count = batch[\"image\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=current_lr)\n",
        "\n",
        "    # Compute metrics for the epoch\n",
        "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
        "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
        "    similarity_matrix = compute_cosine_similarity(all_image_embeddings, all_text_embeddings)\n",
        "    precision, recall = precision_recall(similarity_matrix, k=5)\n",
        "\n",
        "    return loss_meter, precision, recall\n",
        "\n",
        "def validate(model, valid_loader):\n",
        "    model.eval()\n",
        "    loss_meter = AvgMeter()\n",
        "    all_image_embeddings = []\n",
        "    all_text_embeddings = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, total=len(valid_loader), colour=\"blue\"):\n",
        "            batch = {k: v.to(CFG.device, non_blocking=True) for k, v in batch.items() if k != \"caption\"}\n",
        "            loss = model(batch)\n",
        "\n",
        "            # Get embeddings for metrics\n",
        "            image_features = model.image_encoder(batch[\"image\"])\n",
        "            text_features = model.text_encoder(\n",
        "                input_ids=batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"]\n",
        "            )\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "            all_image_embeddings.append(image_embeddings.cpu())\n",
        "            all_text_embeddings.append(text_embeddings.cpu())\n",
        "\n",
        "            count = batch[\"image\"].size(0)\n",
        "            loss_meter.update(loss.item(), count)\n",
        "\n",
        "    # Compute metrics\n",
        "    all_image_embeddings = torch.cat(all_image_embeddings, dim=0)\n",
        "    all_text_embeddings = torch.cat(all_text_embeddings, dim=0)\n",
        "    similarity_matrix = compute_cosine_similarity(all_image_embeddings, all_text_embeddings)\n",
        "    precision, recall = precision_recall(similarity_matrix, k=5)\n",
        "\n",
        "    return loss_meter, precision, recall\n",
        "\n",
        "def main():\n",
        "    print(\"Starting training...\")\n",
        "    train_df, valid_df = make_train_valid_dfs()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "\n",
        "    train_loader = build_loaders(train_df, tokenizer, mode=\"train\")\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(\n",
        "            model.image_projection.parameters(), model.text_projection.parameters()\n",
        "        ), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay},\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
        "    )\n",
        "\n",
        "    # Initialize lists to store metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_precisions = []\n",
        "    train_recalls = []\n",
        "    val_precisions = []\n",
        "    val_recalls = []\n",
        "\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"\\nStarting Epoch {epoch + 1}...\")\n",
        "\n",
        "        # Training\n",
        "        train_loss, train_precision, train_recall = train(model, train_loader, optimizer, lr_scheduler, step=\"epoch\")\n",
        "        train_losses.append(train_loss.avg)\n",
        "        train_precisions.append(train_precision)\n",
        "        train_recalls.append(train_recall)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss, val_precision, val_recall = validate(model, valid_loader)\n",
        "        val_losses.append(valid_loss.avg)\n",
        "        val_precisions.append(val_precision)\n",
        "        val_recalls.append(val_recall)\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{CFG.epochs}\")\n",
        "        print(f\"Training Loss: {train_loss.avg:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "        print(f\"Validation Loss: {valid_loss.avg:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n",
        "\n",
        "        lr_scheduler.step(valid_loss.avg)\n",
        "\n",
        "        if valid_loss.avg < best_loss:\n",
        "            best_loss = valid_loss.avg\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(f\"Saved best model with validation loss: {best_loss:.4f}\")\n",
        "\n",
        "        # Plot Loss and Metrics\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot Training and Validation Loss\n",
        "        plt.subplot(1, 3, 1)\n",
        "        epochs_range = range(1, epoch + 2)\n",
        "        plt.plot(epochs_range, train_losses, label=\"Train Loss\", marker=\"o\", color=\"blue\")\n",
        "        plt.plot(epochs_range, val_losses, label=\"Val Loss\", marker=\"x\", color=\"green\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Training Metrics\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.plot(epochs_range, train_precisions, label=\"Precision@5\", marker=\"o\", color=\"orange\")\n",
        "        plt.plot(epochs_range, train_recalls, label=\"Recall@5\", marker=\"x\", color=\"purple\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Metrics\")\n",
        "        plt.title(\"Training Precision and Recall\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Plot Validation Metrics\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot(epochs_range, val_precisions, label=\"Precision@5\", marker=\"o\", color=\"orange\")\n",
        "        plt.plot(epochs_range, val_recalls, label=\"Recall@5\", marker=\"x\", color=\"purple\")\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Metrics\")\n",
        "        plt.title(\"Validation Precision and Recall\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYO5rLBH-FxN",
        "outputId": "784024c0-be73-4420-b1af-05aad1dbc37d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Loaded 88641 valid caption entries\n",
            "Found 70919 valid images out of 70919\n",
            "Found 17722 valid images out of 17722\n",
            "\n",
            "Starting Epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|\u001b[32m▏         \u001b[0m| 28/2217 [02:59<3:13:16,  5.30s/it, lr=0.0001, train_loss=12]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the Google Drive path for saving the model\n",
        "save_path = \"/content/drive/MyDrive/Bangla Image dataset with caption/banglaclipcombined.pt\"\n",
        "\n",
        "# Define your model class (ensure this matches the architecture used during training)\n",
        "model = CLIPModel().to(CFG.device)  # Replace CLIPModel with your actual model class\n",
        "\n",
        "# Load the trained model from the saved state dictionary\n",
        "checkpoint_path = \"/content/best_model.pth\"\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "# Save the model's state dictionary to the specified Google Drive path\n",
        "torch.save(model.state_dict(), save_path)\n",
        "\n",
        "print(f\"Model saved successfully at: {save_path}\")\n"
      ],
      "metadata": {
        "id": "JvZ37sXX3grB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJxU0NebXSLj"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(dataframe, model_path):\n",
        "    \"\"\"\n",
        "    Loads a model and generates image embeddings for the provided dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataframe: DataFrame containing image file names.\n",
        "    - model_path: Path to the saved model file.\n",
        "\n",
        "    Returns:\n",
        "    - model: Loaded CLIPModel instance.\n",
        "    - image_embeddings: Tensor of image embeddings for the dataset.\n",
        "    \"\"\"\n",
        "    # Load tokenizer and model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    # Prepare data loader\n",
        "    transforms = get_transforms(mode=\"valid\")\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=CFG.batch_size, num_workers=CFG.num_workers, shuffle=False\n",
        "    )\n",
        "\n",
        "    # Generate image embeddings\n",
        "    image_embeddings_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, colour=\"yellow\", desc=\"Generating image embeddings\"):\n",
        "            images = batch[\"image\"].to(CFG.device)\n",
        "            image_features = model.image_encoder(images)\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            image_embeddings_list.append(image_embeddings)\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    image_embeddings = torch.cat(image_embeddings_list, dim=0)\n",
        "    return model, image_embeddings\n",
        "\n",
        "\n",
        "# Interface Function to Use find_matches\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure the validation DataFrame and model are ready\n",
        "    _, valid_df = make_train_valid_dfs()\n",
        "\n",
        "    # Generate the model and image embeddings\n",
        "    model, image_embeddings = get_image_embeddings(valid_df, \"best_model.pth\")\n",
        "\n",
        "    # Define the Bangla text query and display matches\n",
        "    query = \"কক্সবাজারের সমুদ্র সৈকত\"  # Bangla query for \"cycle\"\n",
        "    find_matches(\n",
        "        model=model,\n",
        "        image_embeddings=image_embeddings,\n",
        "        query=query,\n",
        "        image_filenames=valid_df[\"image\"].values,\n",
        "        n=9,\n",
        "    )\n"
      ],
      "metadata": {
        "id": "smKkrI30-kr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PDIebVgkCD-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-gbHD-E7CDnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OXi50mMqCDd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}