{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQJY3d07U31q",
        "outputId": "4f856aa0-84ee-482a-f656-13a3fb3584ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.27.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers sentencepiece torch torchvision albumentations timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpZhkf6HJ3oH",
        "outputId": "d43771f4-fa31-4219-b408-66a3476b8ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import albumentations as A\n",
        "from transformers import AutoTokenizer\n",
        "from PIL import Image\n",
        "import timm\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "W3euTTANJvuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import cv2\n",
        "import itertools\n"
      ],
      "metadata": {
        "id": "QqYIssTPWDeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih1A2_PIWcE7",
        "outputId": "4f2f87d0-bb16-4f32-c6b3-4ef021d2ba1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import albumentations as A\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import cv2\n",
        "import itertools\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "b7Nygf1bkmRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration Class\n",
        "class CFG:\n",
        "    model_name = \"resnet50\"\n",
        "    text_encoder_model = \"csebuetnlp/banglabert\"\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    batch_size = 16\n",
        "    size = 224\n",
        "    image_embedding = 2048\n",
        "    text_embedding = 768\n",
        "    projection_dim = 512\n",
        "    max_length = 2\n",
        "    temperature = 0.07\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    head_lr = 1e-3\n",
        "    weight_decay = 1e-4\n",
        "    patience = 2\n",
        "    factor = 0.5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    epochs = 7"
      ],
      "metadata": {
        "id": "WLtfFRAXkrEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the image folders and caption files\n",
        "image_folder1 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/Pictures'\n",
        "caption_file1 = '/content/drive/MyDrive/Bangla Image dataset with caption/BNATURE/caption/captions.json'\n",
        "image_files_key1 = 'caption_id'\n",
        "caption_key1 = 'bengali_caption'\n",
        "\n",
        "image_folder2 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/images'\n",
        "caption_file2 = '/content/drive/MyDrive/Bangla Image dataset with caption/Bangla Lekha 2.0/captions.json'\n",
        "image_files_key2 = 'filename'\n",
        "caption_key2 = 'caption'\n",
        "\n",
        "image_folder3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "caption_file3 = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/BAN-Cap_captiondata.json'\n",
        "image_files_key3 = 'caption_id'\n",
        "caption_key3 = 'bengali_caption'"
      ],
      "metadata": {
        "id": "Wzim4A1mWL27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a single dataset\n",
        "def load_dataset(image_folder, caption_file, image_files_key, caption_key):\n",
        "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "        captions_data = json.load(f)\n",
        "\n",
        "    images, captions = [], []\n",
        "    for item in captions_data:\n",
        "        image_file = os.path.join(image_folder, item[image_files_key])\n",
        "        caption = item[caption_key]\n",
        "        if os.path.exists(image_file):  # Ensure the image file exists\n",
        "            images.append(image_file)\n",
        "            captions.append(caption)\n",
        "\n",
        "    return {\"image\": images, \"caption\": captions}\n",
        "\n",
        "# Load all datasets\n",
        "data1 = load_dataset(image_folder1, caption_file1, image_files_key1, caption_key1)\n",
        "data2 = load_dataset(image_folder2, caption_file2, image_files_key2, caption_key2)\n",
        "data3 = load_dataset(image_folder3, caption_file3, image_files_key3, caption_key3)\n",
        "\n",
        "# Combine all datasets\n",
        "all_images = data1[\"image\"] + data2[\"image\"] + data3[\"image\"]\n",
        "all_captions = data1[\"caption\"] + data2[\"caption\"] + data3[\"caption\"]\n"
      ],
      "metadata": {
        "id": "AMXprWdIWQoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(TorchDataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = captions\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Tokenize captions with consistent output\n",
        "        encoded = self.tokenizer(\n",
        "            self.captions[idx],\n",
        "            padding=\"max_length\",  # Ensure padding to max_length\n",
        "            truncation=True,       # Ensure truncation to max_length\n",
        "            max_length=CFG.max_length,\n",
        "            return_tensors=None,   # Return plain lists, not tensors\n",
        "        )\n",
        "\n",
        "        # Convert tokenized outputs to tensors\n",
        "        input_ids = torch.tensor(encoded[\"input_ids\"], dtype=torch.long)\n",
        "        attention_mask = torch.tensor(encoded[\"attention_mask\"], dtype=torch.long)\n",
        "\n",
        "        # Validate shapes\n",
        "        assert input_ids.ndim == 1, f\"Input IDs have incorrect shape {input_ids.shape}\"\n",
        "        assert attention_mask.ndim == 1, f\"Attention Mask has incorrect shape {attention_mask.shape}\"\n",
        "\n",
        "        # Load and process the image\n",
        "        image = cv2.imread(self.image_filenames[idx])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)[\"image\"]\n",
        "\n",
        "        return {\n",
        "            \"image\": torch.tensor(image).permute(2, 0, 1).float(),\n",
        "            \"input_ids\": input_ids,  # Correct 1D tensor\n",
        "            \"attention_mask\": attention_mask,  # Correct 1D tensor\n",
        "            \"caption\": self.captions[idx],\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n"
      ],
      "metadata": {
        "id": "ss2JNy8ZsS1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    try:\n",
        "        # Stack images\n",
        "        images = torch.stack([item[\"image\"] for item in batch])\n",
        "\n",
        "        # Validate shapes of input tensors\n",
        "        for i, item in enumerate(batch):\n",
        "            assert item[\"input_ids\"].ndim == 1, f\"Input IDs of item {i} have incorrect shape {item['input_ids'].shape}\"\n",
        "            assert item[\"attention_mask\"].ndim == 1, f\"Attention Mask of item {i} has incorrect shape {item['attention_mask'].shape}\"\n",
        "\n",
        "        # Pad sequences (input_ids and attention_mask)\n",
        "        input_ids = pad_sequence(\n",
        "            [item[\"input_ids\"] for item in batch], batch_first=True, padding_value=0\n",
        "        )  # Shape: [batch_size, seq_length]\n",
        "\n",
        "        attention_mask = pad_sequence(\n",
        "            [item[\"attention_mask\"] for item in batch], batch_first=True, padding_value=0\n",
        "        )  # Shape: [batch_size, seq_length]\n",
        "\n",
        "        captions = [item[\"caption\"] for item in batch]\n",
        "\n",
        "    except AssertionError as e:\n",
        "        print(f\"Error in collate function: {e}\")\n",
        "        for i, item in enumerate(batch):\n",
        "            print(f\"Item {i} Input IDs Shape: {item['input_ids'].shape}\")\n",
        "            print(f\"Item {i} Attention Mask Shape: {item['attention_mask'].shape}\")\n",
        "        raise e\n",
        "\n",
        "    return {\n",
        "        \"image\": images,\n",
        "        \"input_ids\": input_ids,  # Shape: [batch_size, seq_length]\n",
        "        \"attention_mask\": attention_mask,  # Shape: [batch_size, seq_length]\n",
        "        \"caption\": captions,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "jTV0fN1XsZrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transforms\n",
        "def get_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "    ])"
      ],
      "metadata": {
        "id": "UFMGDkmxlfqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Encoder\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KWurp9x7XKws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Log input shapes for debugging\n",
        "        print(f\"TextEncoder Input IDs Shape: {input_ids.shape}\")  # Expected: [batch_size, seq_length]\n",
        "        print(f\"TextEncoder Attention Mask Shape: {attention_mask.shape}\")  # Expected: [batch_size, seq_length]\n",
        "\n",
        "        # Pass inputs through the transformer model\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state  # Shape: [batch_size, seq_length, hidden_dim]\n",
        "\n",
        "        # Return features for the target token (e.g., CLS token)\n",
        "        return last_hidden_state[:, self.target_token_idx, :]  # Shape: [batch_size, hidden_dim]\n"
      ],
      "metadata": {
        "id": "X1LsmmwLrmjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Projection Head\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, projection_dim=CFG.projection_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "QDrof7DDXYFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self, temperature=CFG.temperature):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=CFG.image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=CFG.text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Log input shapes for debugging\n",
        "        print(f\"Image Shape: {batch['image'].shape}\")\n",
        "        print(f\"Input IDs Shape: {batch['input_ids'].shape}\")\n",
        "        print(f\"Attention Mask Shape: {batch['attention_mask'].shape}\")\n",
        "\n",
        "        # Encode image and text\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "\n",
        "        # Project embeddings\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        # Compute logits and loss\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "\n",
        "        targets = F.softmax((images_similarity + texts_similarity) / 2 * self.temperature, dim=-1)\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "kMbnNh3Tq3ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    return loss.mean() if reduction == \"mean\" else loss\n"
      ],
      "metadata": {
        "id": "NPmedzmUXrwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(CFG.size, CFG.size, always_apply=True),  # Resize all images to CFG.size\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "nqOAXAHOYwtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect Dataset Output\n",
        "dataset = CLIPDataset(all_images, all_captions, tokenizer, transforms)\n",
        "for i in range(5):  # Check the first 5 samples\n",
        "    sample = dataset[i]\n",
        "    print(f\"Sample {i} - Input IDs shape: {sample['input_ids'].shape}, Attention Mask shape: {sample['attention_mask'].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46ZLVTYboC-p",
        "outputId": "a81c62f3-2aab-43d8-9e38-6a25b90e286b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0 - Input IDs shape: torch.Size([2]), Attention Mask shape: torch.Size([2])\n",
            "Sample 1 - Input IDs shape: torch.Size([2]), Attention Mask shape: torch.Size([2])\n",
            "Sample 2 - Input IDs shape: torch.Size([2]), Attention Mask shape: torch.Size([2])\n",
            "Sample 3 - Input IDs shape: torch.Size([2]), Attention Mask shape: torch.Size([2])\n",
            "Sample 4 - Input IDs shape: torch.Size([2]), Attention Mask shape: torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Dataset\n",
        "for i in range(5):\n",
        "    sample = dataset[i]\n",
        "    print(f\"Sample {i} Input IDs Shape: {sample['input_ids'].shape}\")  # Expected: [CFG.max_length]\n",
        "    print(f\"Sample {i} Attention Mask Shape: {sample['attention_mask'].shape}\")  # Expected: [CFG.max_length]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxfQ3Py0qRRS",
        "outputId": "20fe6daa-6c16-4ea5-ae03-7564237a1f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0 Input IDs Shape: torch.Size([2])\n",
            "Sample 0 Attention Mask Shape: torch.Size([2])\n",
            "Sample 1 Input IDs Shape: torch.Size([2])\n",
            "Sample 1 Attention Mask Shape: torch.Size([2])\n",
            "Sample 2 Input IDs Shape: torch.Size([2])\n",
            "Sample 2 Attention Mask Shape: torch.Size([2])\n",
            "Sample 3 Input IDs Shape: torch.Size([2])\n",
            "Sample 3 Attention Mask Shape: torch.Size([2])\n",
            "Sample 4 Input IDs Shape: torch.Size([2])\n",
            "Sample 4 Attention Mask Shape: torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test DataLoader\n",
        "for batch in train_loader:\n",
        "    print(f\"Batch Input IDs Shape: {batch['input_ids'].shape}\")  # Expected: [batch_size, CFG.max_length]\n",
        "    print(f\"Batch Attention Mask Shape: {batch['attention_mask'].shape}\")  # Expected: [batch_size, CFG.max_length]\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uIwdJrVrIES",
        "outputId": "09d4c569-dfeb-486f-ffa2-5295ff66869f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Index 0: Image Shape torch.Size([3, 224, 224]), Input IDs Length 70\n",
            "Batch Index 1: Image Shape torch.Size([3, 224, 224]), Input IDs Length 70\n",
            "Batch Index 2: Image Shape torch.Size([3, 224, 224]), Input IDs Length 70\n",
            "Batch Index 3: Image Shape torch.Size([3, 224, 224]), Input IDs Length 70\n",
            "Batch Input IDs Shape: torch.Size([4, 70])\n",
            "Batch Attention Mask Shape: torch.Size([4, 70])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    model.train()\n",
        "    scaler = torch.amp.GradScaler()\n",
        "    loss_meter = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, total=len(train_loader)):\n",
        "        # Log input shapes\n",
        "        print(f\"Batch Image Shape: {batch['image'].shape}\")  # Expected: [batch_size, 3, CFG.size, CFG.size]\n",
        "        print(f\"Batch Input IDs Shape: {batch['input_ids'].shape}\")  # Expected: [batch_size, CFG.max_length]\n",
        "        print(f\"Batch Attention Mask Shape: {batch['attention_mask'].shape}\")  # Expected: [batch_size, CFG.max_length]\n",
        "\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            loss = model(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        loss_meter += loss.item()\n",
        "\n",
        "    return loss_meter / len(train_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def valid_epoch(model, valid_loader):\n",
        "    model.eval()\n",
        "    loss_meter = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader, total=len(valid_loader)):\n",
        "            batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "            loss = model(batch)\n",
        "            loss_meter += loss.item()\n",
        "    return loss_meter / len(valid_loader)\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "    transforms = get_transforms()\n",
        "    dataset = CLIPDataset(all_images, all_captions, tokenizer, transforms)\n",
        "\n",
        "    # Split dataset\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "    # DataLoader with custom collate_fn\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4, collate_fn=custom_collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4, collate_fn=custom_collate_fn)\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()),\n",
        "         \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.0)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor)\n",
        "    step = \"epoch\"\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{CFG.epochs}\")\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        valid_loss = valid_epoch(model, valid_loader)\n",
        "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), \"best_clip_model_bangla.pt\")\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        if step == \"epoch\":\n",
        "            lr_scheduler.step(valid_loss)\n",
        "\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "S_wN0PvhlRog",
        "outputId": "252108e5-cc3d-4c73-af03-d2db39eaf2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/2413 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-281-f3c98c09fde2>\", line 23, in __getitem__\n    assert input_ids.ndim == 1, f\"Input IDs have incorrect shape {input_ids.shape}\"\nAssertionError: Input IDs have incorrect shape torch.Size([2, 2])\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-293-cea13963d435>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Execute the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-293-cea13963d435>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{CFG.epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-293-cea13963d435>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, step)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_meter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Log input shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Batch Image Shape: {batch['image'].shape}\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expected: [batch_size, 3, CFG.size, CFG.size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 351, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 420, in <listcomp>\n    return [self.dataset[self.indices[idx]] for idx in indices]\n  File \"<ipython-input-281-f3c98c09fde2>\", line 23, in __getitem__\n    assert input_ids.ndim == 1, f\"Input IDs have incorrect shape {input_ids.shape}\"\nAssertionError: Input IDs have incorrect shape torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmpc0cgirWoA"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX8wC1PO7LhV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming CLIPDataset and CFG have already been defined\n",
        "# Ensure the paths to the dataset are properly set\n",
        "\n",
        "# Load and prepare the dataset\n",
        "with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Prepare image files and captions\n",
        "image_files = [item['caption_id'].split('#')[0] for item in captions_data]\n",
        "captions = [item['bengali_caption'] for item in captions_data]\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "transforms = get_transforms()\n",
        "\n",
        "# Create the dataset\n",
        "dataset = CLIPDataset(image_files, captions, tokenizer, transforms)\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "_, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "# Define the function to get image embeddings\n",
        "def get_image_embeddings(valid_dataset, model_path):\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    valid_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            valid_image_embeddings.append(image_embeddings)\n",
        "\n",
        "    return model, torch.cat(valid_image_embeddings)\n",
        "\n",
        "# Perform inference to get image embeddings from the validation set\n",
        "model, image_embeddings = get_image_embeddings(valid_dataset, \"/content/best_clip_model_bangla.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt  # Add this import\n"
      ],
      "metadata": {
        "id": "Dd7-DzquNn2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matches(model, image_embeddings, query, image_files, n=9):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "\n",
        "    # Encode the query and print to ensure uniqueness\n",
        "    encoded_query = tokenizer([query], return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.max_length)\n",
        "    print(f\"Encoded Query: {encoded_query}\")  # Check if different prompts yield different encodings\n",
        "\n",
        "    batch = {\n",
        "        key: torch.tensor(values).to(CFG.device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "    # Normalize embeddings and print for debugging\n",
        "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "\n",
        "    print(f\"Text Embeddings: {text_embeddings_n}\")  # Check if text embeddings change with different prompts\n",
        "\n",
        "    # Calculate similarity and retrieve top matches\n",
        "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "    values, indices = torch.topk(dot_similarity.squeeze(0), n)\n",
        "\n",
        "    matches = [image_files[idx] for idx in indices]\n",
        "    print(f\"Top match values: {values}\")  # To see if similarity scores vary\n",
        "\n",
        "    # Display the matched images\n",
        "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    for match, ax in zip(matches, axes.flatten()):\n",
        "        image = cv2.imread(f\"{image_folder}/{match}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(image)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yrLIuP7oNu5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiPDBptxrZQG"
      },
      "outputs": [],
      "source": [
        "prompt = \" \"  # Example prompt for \"A dog is playing\"\n",
        "find_matches(model, image_embeddings, prompt, image_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Path to your saved model file\n",
        "model_path = \"best_clip_model_bangla.pt\"  # Replace with your actual model file name if different\n",
        "\n",
        "# Download the file\n",
        "files.download(model_path)\n"
      ],
      "metadata": {
        "id": "mHNFLmICA1kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T390_6NgA2vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}