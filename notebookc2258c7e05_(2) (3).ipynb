{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 39911,
          "sourceType": "datasetVersion",
          "datasetId": 31296
        }
      ],
      "dockerImageVersionId": 30085,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e546377cb8246fa891ae5ce632f99e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58cf55b714114bc78898be9d4ce045bb",
              "IPY_MODEL_f18245f62a7f4695acb0cbfc07d21d72",
              "IPY_MODEL_5f5d6325ddb14a05a59eb73a8e08d537"
            ],
            "layout": "IPY_MODEL_4452f1c327ce4e14bfe1de3d76ec0bf2"
          }
        },
        "58cf55b714114bc78898be9d4ce045bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_793aaf380bd24087b9b72be85ab18c45",
            "placeholder": "​",
            "style": "IPY_MODEL_0ae72fa7e8e44075bb8e97ee085d9450",
            "value": ""
          }
        },
        "f18245f62a7f4695acb0cbfc07d21d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c9cca3c5ac44b5aa8c53e9a6d207d18",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d547067f612f437e91836b9cd8944db5",
            "value": 1
          }
        },
        "5f5d6325ddb14a05a59eb73a8e08d537": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01422a1bf4534285b021832188639c92",
            "placeholder": "​",
            "style": "IPY_MODEL_d2e098228f264ecc90c269746d219e47",
            "value": " 21996/? [01:27&lt;00:00, 327.59it/s]"
          }
        },
        "4452f1c327ce4e14bfe1de3d76ec0bf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "793aaf380bd24087b9b72be85ab18c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ae72fa7e8e44075bb8e97ee085d9450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c9cca3c5ac44b5aa8c53e9a6d207d18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "d547067f612f437e91836b9cd8944db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01422a1bf4534285b021832188639c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2e098228f264ecc90c269746d219e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "3f-hIu12D6q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch\n",
        "!pip install --upgrade safetensors\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:56:42.406164Z",
          "iopub.execute_input": "2024-12-23T08:56:42.406566Z",
          "iopub.status.idle": "2024-12-23T08:58:45.756303Z",
          "shell.execute_reply.started": "2024-12-23T08:56:42.406481Z",
          "shell.execute_reply": "2024-12-23T08:58:45.755286Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csZoTZRuD6rO",
        "outputId": "5f67d5fb-2e77-4a9c-90fe-04955aee8506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision timm safetensors\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install timm safetensors\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:58:45.758160Z",
          "iopub.execute_input": "2024-12-23T08:58:45.758420Z",
          "iopub.status.idle": "2024-12-23T08:59:29.370155Z",
          "shell.execute_reply.started": "2024-12-23T08:58:45.758394Z",
          "shell.execute_reply": "2024-12-23T08:59:29.369283Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNNfSZ5ID6rU",
        "outputId": "2557f1b1-b39a-43f0-d3a7-ff70bbfb5980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.5.1+cu118\n",
            "Uninstalling torch-2.5.1+cu118:\n",
            "  Successfully uninstalled torch-2.5.1+cu118\n",
            "Found existing installation: torchvision 0.20.1+cu118\n",
            "Uninstalling torchvision-0.20.1+cu118:\n",
            "  Successfully uninstalled torchvision-0.20.1+cu118\n",
            "Found existing installation: timm 1.0.12\n",
            "Uninstalling timm-1.0.12:\n",
            "  Successfully uninstalled timm-1.0.12\n",
            "Found existing installation: safetensors 0.4.5\n",
            "Uninstalling safetensors-0.4.5:\n",
            "  Successfully uninstalled safetensors-0.4.5\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp310-cp310-linux_x86_64.whl (838.3 MB)\n",
            "Collecting torchvision\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp310-cp310-linux_x86_64.whl (6.5 MB)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.2.1 requires safetensors>=0.4.3, which is not installed.\n",
            "peft 0.14.0 requires safetensors, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.5.1+cu118 torchvision-0.20.1+cu118\n",
            "Collecting timm\n",
            "  Using cached timm-1.0.12-py3-none-any.whl.metadata (51 kB)\n",
            "Collecting safetensors\n",
            "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
            "Using cached timm-1.0.12-py3-none-any.whl (2.4 MB)\n",
            "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "Installing collected packages: safetensors, timm\n",
            "Successfully installed safetensors-0.4.5 timm-1.0.12\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch safetensors timm\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:59:29.371576Z",
          "iopub.execute_input": "2024-12-23T08:59:29.371824Z",
          "iopub.status.idle": "2024-12-23T08:59:34.383147Z",
          "shell.execute_reply.started": "2024-12-23T08:59:29.371800Z",
          "shell.execute_reply": "2024-12-23T08:59:34.382298Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCQmJranD6rW",
        "outputId": "07ca494b-4d01-4021-d3d4-7527ec2a0eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.5.1+cu118\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu11, nvidia-cuda-cupti-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-runtime-cu11, nvidia-cudnn-cu11, nvidia-cufft-cu11, nvidia-curand-cu11, nvidia-cusolver-cu11, nvidia-cusparse-cu11, nvidia-nccl-cu11, nvidia-nvtx-cu11, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, fastai, peft, sentence-transformers, timm, torchaudio, torchvision\n",
            "---\n",
            "Name: safetensors\n",
            "Version: 0.4.5\n",
            "Summary: \n",
            "Home-page: https://github.com/huggingface/safetensors\n",
            "Author: \n",
            "Author-email: Nicolas Patry <patry.nicolas@protonmail.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: accelerate, diffusers, peft, timm, transformers\n",
            "---\n",
            "Name: timm\n",
            "Version: 1.0.12\n",
            "Summary: PyTorch Image Models\n",
            "Home-page: https://github.com/huggingface/pytorch-image-models\n",
            "Author: \n",
            "Author-email: Ross Wightman <ross@huggingface.co>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: huggingface_hub, pyyaml, safetensors, torch, torchvision\n",
            "Required-by: \n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:59:34.384586Z",
          "iopub.execute_input": "2024-12-23T08:59:34.384847Z",
          "iopub.status.idle": "2024-12-23T08:59:46.196683Z",
          "shell.execute_reply.started": "2024-12-23T08:59:34.384820Z",
          "shell.execute_reply": "2024-12-23T08:59:46.195798Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0N4TrHQD6rX",
        "outputId": "978fc924-fbb8-4ff5-d220-42254210913d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:59:46.199611Z",
          "iopub.execute_input": "2024-12-23T08:59:46.199877Z",
          "iopub.status.idle": "2024-12-23T08:59:51.325840Z",
          "shell.execute_reply.started": "2024-12-23T08:59:46.199851Z",
          "shell.execute_reply": "2024-12-23T08:59:51.325011Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCDYishjD6rZ",
        "outputId": "24dbba41-3c68-4aa3-a115-a2278964669f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install safetensors==0.3.0\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:59:51.329035Z",
          "iopub.execute_input": "2024-12-23T08:59:51.329384Z",
          "iopub.status.idle": "2024-12-23T08:59:57.288299Z",
          "shell.execute_reply.started": "2024-12-23T08:59:51.329355Z",
          "shell.execute_reply": "2024-12-23T08:59:57.287579Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WV-pbIm_D6ra",
        "outputId": "bd77d2a1-29d6-415a-a878-dd03059692c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting safetensors==0.3.0\n",
            "  Using cached safetensors-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached safetensors-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Installing collected packages: safetensors\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.4.5\n",
            "    Uninstalling safetensors-0.4.5:\n",
            "      Successfully uninstalled safetensors-0.4.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "accelerate 1.2.1 requires safetensors>=0.4.3, but you have safetensors 0.3.0 which is incompatible.\n",
            "diffusers 0.31.0 requires safetensors>=0.3.1, but you have safetensors 0.3.0 which is incompatible.\n",
            "transformers 4.47.1 requires safetensors>=0.4.1, but you have safetensors 0.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed safetensors-0.3.0\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPpnm23hFIYU",
        "outputId": "bcd937f9-3b64-4194-a6d5-e4a850097986"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Collecting safetensors>=0.4.1 (from transformers)\n",
            "  Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Using cached safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
            "Installing collected packages: safetensors\n",
            "  Attempting uninstall: safetensors\n",
            "    Found existing installation: safetensors 0.3.0\n",
            "    Uninstalling safetensors-0.3.0:\n",
            "      Successfully uninstalled safetensors-0.3.0\n",
            "Successfully installed safetensors-0.4.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade safetensors transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKeaLG5AFuP7",
        "outputId": "dee55695-4b4e-4cc8-aef2-c7d2030003d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.89)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.87)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.10/dist-packages (from torch) (11.11.3.6)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.10/dist-packages (from torch) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.0.86)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.1.48)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.7.5.86)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.10/dist-packages (from torch) (11.8.86)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install dependencies (if any)\n",
        "!pip install --upgrade datasets transformers safetensors\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Step 3: Connect to Hugging Face dataset for image captions\n",
        "# Load Flickr30k dataset with streaming enabled to prevent memory overflow\n",
        "dataset = load_dataset(\"nlphuji/flickr30k\", streaming=True)\n",
        "\n",
        "# Display dataset keys to check available splits\n",
        "print(\"Available splits:\", dataset.keys())\n",
        "\n",
        "# Dynamically select the first available split\n",
        "split_name = list(dataset.keys())[0]\n",
        "print(f\"Using split: {split_name}\")\n",
        "\n",
        "# Initialize lists for image filenames and captions\n",
        "filenames = []  # To store filenames (used as image paths)\n",
        "captions = []   # To store captions\n",
        "\n",
        "# Process the dataset to extract filenames and captions\n",
        "for example in tqdm(dataset[split_name]):\n",
        "    filenames.append(example['filename'])  # Use 'filename' for image column\n",
        "    captions.append(example['caption'])   # Use 'caption' for caption column\n",
        "\n",
        "# Create a DataFrame with 'image' and 'caption' columns\n",
        "caption_data = pd.DataFrame({'image': filenames, 'caption': captions})\n",
        "\n",
        "# Ensure that 'image' and 'caption' columns are present\n",
        "print(\"Preview of the caption dataset:\")\n",
        "print(caption_data.head())\n",
        "\n",
        "# Save the processed captions to a CSV file\n",
        "output_path = \"captions.csv\"\n",
        "caption_data.to_csv(output_path, index=False)\n",
        "print(f\"Processed captions saved to {output_path}\")\n",
        "\n",
        "# Perform basic analysis of the dataset\n",
        "print(\"\\nBasic analysis of the dataset:\")\n",
        "print(caption_data.describe())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989,
          "referenced_widgets": [
            "7e546377cb8246fa891ae5ce632f99e8",
            "58cf55b714114bc78898be9d4ce045bb",
            "f18245f62a7f4695acb0cbfc07d21d72",
            "5f5d6325ddb14a05a59eb73a8e08d537",
            "4452f1c327ce4e14bfe1de3d76ec0bf2",
            "793aaf380bd24087b9b72be85ab18c45",
            "0ae72fa7e8e44075bb8e97ee085d9450",
            "6c9cca3c5ac44b5aa8c53e9a6d207d18",
            "d547067f612f437e91836b9cd8944db5",
            "01422a1bf4534285b021832188639c92",
            "d2e098228f264ecc90c269746d219e47"
          ]
        },
        "id": "wi_FGCkVQgb3",
        "outputId": "57758bee-a5e6-4bf5-e422-2451c00cc50d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Available splits: dict_keys(['test'])\n",
            "Using split: test\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e546377cb8246fa891ae5ce632f99e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preview of the caption dataset:\n",
            "            image                                            caption\n",
            "0  1000092795.jpg  [Two young guys with shaggy hair look at their...\n",
            "1    10002456.jpg  [Several men in hard hats are operating a gian...\n",
            "2  1000268201.jpg  [A child in a pink dress is climbing up a set ...\n",
            "3  1000344755.jpg  [Someone in a blue shirt and hat is standing o...\n",
            "4  1000366164.jpg  [Two men, one in a gray shirt, one in a black ...\n",
            "Processed captions saved to captions.csv\n",
            "\n",
            "Basic analysis of the dataset:\n",
            "                image                                            caption\n",
            "count           31014                                              31014\n",
            "unique          31014                                              31014\n",
            "top     998845445.jpg  [A man in shorts and a Hawaiian shirt leans ov...\n",
            "freq                1                                                  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from tqdm.autonotebook import tqdm\n",
        "import albumentations as A\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import timm\n",
        "from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer"
      ],
      "metadata": {
        "papermill": {
          "duration": 3.862689,
          "end_time": "2021-04-05T08:01:49.835804",
          "exception": false,
          "start_time": "2021-04-05T08:01:45.973115",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T08:59:57.289779Z",
          "iopub.execute_input": "2024-12-23T08:59:57.290124Z",
          "iopub.status.idle": "2024-12-23T09:00:01.402640Z",
          "shell.execute_reply.started": "2024-12-23T08:59:57.290087Z",
          "shell.execute_reply": "2024-12-23T09:00:01.401778Z"
        },
        "id": "LOktRN_vD6rb"
      },
      "outputs": [],
      "execution_count": 65
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some pre-preocessing"
      ],
      "metadata": {
        "id": "rNrKfrXXD6sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    # Debug mode\n",
        "    debug = False\n",
        "\n",
        "    # Hugging Face dataset configuration\n",
        "    image_path = None  # Images are streamed directly from the Hugging Face dataset\n",
        "    captions_path = \"./\"  # Path for saving processed captions (current directory)\n",
        "\n",
        "    # DataLoader configuration\n",
        "    batch_size = 32\n",
        "    num_workers = 4\n",
        "\n",
        "    # Learning rates and training parameters\n",
        "    head_lr = 1e-3\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    weight_decay = 1e-3\n",
        "    patience = 1\n",
        "    factor = 0.8\n",
        "    epochs = 2\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_name = 'resnet50'  # Image encoder model\n",
        "    image_embedding = 2048  # Output size of the image encoder\n",
        "    text_encoder_model = \"distilbert-base-uncased\"  # Text encoder model\n",
        "    text_embedding = 768  # Output size of the text encoder\n",
        "    text_tokenizer = \"distilbert-base-uncased\"  # Tokenizer for text preprocessing\n",
        "    max_length = 200  # Maximum length of text tokens\n",
        "\n",
        "    # Encoder options\n",
        "    pretrained = True  # Use pretrained weights for both encoders\n",
        "    trainable = True  # Allow fine-tuning of both encoders\n",
        "    temperature = 1.0  # Temperature for contrastive loss\n",
        "\n",
        "    # Image preprocessing configuration\n",
        "    size = 224  # Image resizing dimension\n",
        "\n",
        "    # Projection head configuration\n",
        "    num_projection_layers = 1  # Layers in projection head\n",
        "    projection_dim = 256  # Dimension of the projection layer\n",
        "    dropout = 0.1  # Dropout for regularization\n"
      ],
      "metadata": {
        "id": "K3ePR5CkLnES"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.012746,
          "end_time": "2021-04-05T08:01:51.74007",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.727324",
          "status": "completed"
        },
        "tags": [],
        "id": "C4S3rKaLD6sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgMeter:\n",
        "    \"\"\"\n",
        "    Tracks and updates the average of a metric during training or evaluation.\n",
        "    \"\"\"\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the metric values.\"\"\"\n",
        "        self.avg = 0.0\n",
        "        self.sum = 0.0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        \"\"\"\n",
        "        Updates the metric with a new value.\n",
        "\n",
        "        Args:\n",
        "            val (float): The new value to update.\n",
        "            count (int): The weight or count for the value (default: 1).\n",
        "        \"\"\"\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Returns a formatted string representation of the metric.\n",
        "        \"\"\"\n",
        "        return f\"{self.name}: {self.avg:.4f}\"\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    \"\"\"\n",
        "    Retrieves the current learning rate from the optimizer.\n",
        "\n",
        "    Args:\n",
        "        optimizer (torch.optim.Optimizer): The optimizer instance.\n",
        "\n",
        "    Returns:\n",
        "        float: The current learning rate.\n",
        "    \"\"\"\n",
        "    if not optimizer.param_groups:\n",
        "        raise ValueError(\"Optimizer has no parameter groups.\")\n",
        "    return optimizer.param_groups[0][\"lr\"]\n"
      ],
      "metadata": {
        "id": "S2UNkg0tL79g"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, tokenizer, transforms):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataset: Hugging Face dataset loaded with streaming enabled.\n",
        "            tokenizer: Hugging Face tokenizer for encoding captions.\n",
        "            transforms: Augmentations and preprocessing pipeline for images.\n",
        "        \"\"\"\n",
        "        self.dataset = list(dataset)  # Convert the streaming dataset to a list for indexing\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # Extract captions and encode them\n",
        "        self.captions = [item['caption'] for item in self.dataset]\n",
        "        self.encoded_captions = tokenizer(\n",
        "            self.captions, padding=True, truncation=True, max_length=CFG.max_length, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: values[idx]\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "\n",
        "        # Process image\n",
        "        image_path = self.dataset[idx]['image']['path']  # Get image path from the dataset\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)['image']\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['caption'] = self.captions[idx]\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)\n",
        "\n",
        "\n",
        "def get_transforms(mode=\"train\"):\n",
        "    \"\"\"\n",
        "    Returns a set of transformations to preprocess images for training or evaluation.\n",
        "\n",
        "    Args:\n",
        "        mode (str): Either 'train' or 'val/test' to determine the augmentations.\n",
        "\n",
        "    Returns:\n",
        "        albumentations.Compose: A composition of transformations.\n",
        "    \"\"\"\n",
        "    if mode == \"train\":\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.HorizontalFlip(p=0.5),  # Add data augmentation for training\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        return A.Compose(\n",
        "            [\n",
        "                A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "                A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "            ]\n",
        "        )\n"
      ],
      "metadata": {
        "id": "3_b07vpmMTqg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import timm\n",
        "\n",
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes images into fixed-size vectors using a pretrained model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        \"\"\"\n",
        "        Initializes the ImageEncoder with a specified model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the model (e.g., 'resnet50').\n",
        "            pretrained (bool): Whether to use pretrained weights.\n",
        "            trainable (bool): Whether to allow fine-tuning of the model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        print(f\"Creating model: {model_name} with pretrained={pretrained}\")\n",
        "\n",
        "        # Create a model with no classification head and global pooling\n",
        "        self.model = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0,  # Remove the classification head\n",
        "            global_pool=\"avg\"  # Use average pooling\n",
        "        )\n",
        "        print(\"Model created successfully.\")\n",
        "\n",
        "        # Set trainability for model parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for encoding images.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor representing a batch of images.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoded image features.\n",
        "        \"\"\"\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "V1mzHCP1Mdj4"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertConfig\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encodes text into fixed-size vectors using a pretrained DistilBERT model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        \"\"\"\n",
        "        Initializes the TextEncoder with a specified model.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): Name of the text encoder model (e.g., 'distilbert-base-uncased').\n",
        "            pretrained (bool): Whether to use pretrained weights.\n",
        "            trainable (bool): Whether to allow fine-tuning of the model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if pretrained:\n",
        "            print(f\"Loading pretrained model: {model_name}\")\n",
        "            self.model = DistilBertModel.from_pretrained(model_name)\n",
        "        else:\n",
        "            print(f\"Creating model from config: {model_name}\")\n",
        "            self.model = DistilBertModel(config=DistilBertConfig())\n",
        "\n",
        "        # Set trainability for model parameters\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = trainable\n",
        "\n",
        "        # Use the CLS token's hidden representation as the sentence embedding\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Forward pass for encoding text.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Tokenized input IDs.\n",
        "            attention_mask (torch.Tensor): Attention mask for tokenized inputs.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Encoded text features (CLS token embedding).\n",
        "        \"\"\"\n",
        "        # Get the output from DistilBERT\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the CLS token embedding\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]\n"
      ],
      "metadata": {
        "id": "Li9AlYigMolJ"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A projection head that maps embeddings to a lower-dimensional space,\n",
        "    adding non-linearity and normalization for alignment tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        projection_dim=CFG.projection_dim,\n",
        "        dropout=CFG.dropout\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the ProjectionHead module.\n",
        "\n",
        "        Args:\n",
        "            embedding_dim (int): Input embedding dimension.\n",
        "            projection_dim (int): Output projection dimension.\n",
        "            dropout (float): Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)  # Linear projection\n",
        "        self.gelu = nn.GELU()  # Non-linearity\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)  # Fully connected layer\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)  # Layer normalization for stability\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the projection head.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Projected tensor of shape (batch_size, projection_dim).\n",
        "        \"\"\"\n",
        "        # Apply first linear projection and activation\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "\n",
        "        # Second projection, dropout, and residual connection\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected  # Residual connection\n",
        "\n",
        "        # Apply layer normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "bHzucRauM1Fg"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check the cell below this code block for the continue of the explanations"
      ],
      "metadata": {
        "id": "TP52mpgeD6ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        temperature=CFG.temperature,\n",
        "        image_embedding=CFG.image_embedding,\n",
        "        text_embedding=CFG.text_embedding,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # Getting Image and Text Features\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        # Getting Image and Text Embeddings (with same dimension)\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "\n",
        "        # Calculating the Loss\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax(\n",
        "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
        "        )\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "        return loss.mean()\n",
        "\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025366,
          "end_time": "2021-04-05T08:01:51.972338",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.946972",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T09:00:02.706299Z",
          "iopub.execute_input": "2024-12-23T09:00:02.706565Z",
          "iopub.status.idle": "2024-12-23T09:00:02.720124Z",
          "shell.execute_reply.started": "2024-12-23T09:00:02.706542Z",
          "shell.execute_reply": "2024-12-23T09:00:02.719531Z"
        },
        "id": "CEiN7aS3D6ss"
      },
      "outputs": [],
      "execution_count": 72
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, in the best case scenario, text_embeddings and image_embedding matricies should be the same because they are describing similar things. Let's think now: if this happens, what would the logits matrix be like? Let's see with a simple example!"
      ],
      "metadata": {
        "id": "X7tU7KUQD6st"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8Tedl4FND6su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple Example\n",
        "\n",
        "batch_size = 4\n",
        "dim = 256\n",
        "embeddings = torch.randn(batch_size, dim)\n",
        "out = embeddings @ embeddings.T\n",
        "print(F.softmax(out, dim=-1))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T09:00:02.721108Z",
          "iopub.execute_input": "2024-12-23T09:00:02.721431Z",
          "iopub.status.idle": "2024-12-23T09:00:02.738133Z",
          "shell.execute_reply.started": "2024-12-23T09:00:02.721395Z",
          "shell.execute_reply": "2024-12-23T09:00:02.737377Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I6aqkunD6sv",
        "outputId": "a3dc8c00-127f-4d3a-f0a0-d9ff111e2101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0.],\n",
            "        [0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n"
          ]
        }
      ],
      "execution_count": 73
    },
    {
      "cell_type": "markdown",
      "source": [
        "So logits, in the best case, will be a matrix that if we take its softmax, will have 1.0s in the diagonal (An identity matrix to call it with fancy words!). As the loss function's job is to make model's predictions similar to targets (at least in most cases!), we want such a matrix as our target. That's the reason why we are calculating images_similarity and texts_similarity matrices in the code block above."
      ],
      "metadata": {
        "id": "TsJ35ntgD6sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've got our targets matrix, we will use simple cross entropy to calculate the actual loss. I've written the full matrix form of cross entropy as a function which you can see in the bottom of the code block. Okay! We are done! Wasn't it simple?! Alright, you can ignore the next paragraph but if you are curious, there is an important note in that."
      ],
      "metadata": {
        "id": "5cI27NqOD6sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here's why I didn't use a simpler approach**: I need to admit that there's a simpler way to calculate this loss in PyTorch; by doing this: nn.CrossEntropyLoss()(logits, torch.arange(batch_size)). Why I did not use it here? For 2 reasons. 1- The dataset we are using has multiple captions for a single image; so, there is the possibility that two identical images with their similar captions exist in a batch (it is rare but it can happen). Taking the loss with this easier method will ignore this possibility and the model learns to pull apart two representations (assume them different)  that are actually the same. Obviously, we don't want this to happen so I calculated the whole target matrix in a way that takes care of these edge cases. 2- Doing it the way I did, gave me a better understanding of what is happening in this loss function; so, I thought it would give you a better intuition as well!"
      ],
      "metadata": {
        "id": "4gw3ZNJvD6sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.013097,
          "end_time": "2021-04-05T08:01:51.998635",
          "exception": false,
          "start_time": "2021-04-05T08:01:51.985538",
          "status": "completed"
        },
        "tags": [],
        "id": "f_wCVGAQD6sy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some funtions to help us load train and valid dataloaders, our model and then train and evaluate our model on those. There's not much going on here; just simple training loop and utility functions"
      ],
      "metadata": {
        "id": "D3Sj1eORD6sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Adjusted version of make_train_valid_dfs to handle missing 'id' and 'image' columns\n",
        "\n",
        "def make_train_valid_dfs():\n",
        "    # Load the dataset\n",
        "    dataframe = pd.read_csv(f\"{CFG.captions_path}/captions.csv\")\n",
        "\n",
        "    # Ensure the 'id' column exists, create it if missing\n",
        "    if \"id\" not in dataframe.columns:\n",
        "        dataframe[\"id\"] = np.arange(len(dataframe))\n",
        "\n",
        "    # Ensure the 'image' column exists, raise an error if missing\n",
        "    if \"image\" not in dataframe.columns:\n",
        "        raise KeyError(\"The 'image' column is missing from the dataset. Please ensure it is included in the CSV file.\")\n",
        "\n",
        "    # Split dataset into train and valid sets\n",
        "    max_id = dataframe[\"id\"].max() + 1 if not CFG.debug else 100\n",
        "    image_ids = np.arange(0, max_id)\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    valid_ids = np.random.choice(\n",
        "        image_ids, size=int(0.2 * len(image_ids)), replace=False\n",
        "    )\n",
        "    train_ids = [id_ for id_ in image_ids if id_ not in valid_ids]\n",
        "\n",
        "    train_dataframe = dataframe[dataframe[\"id\"].isin(train_ids)].reset_index(drop=True)\n",
        "    valid_dataframe = dataframe[dataframe[\"id\"].isin(valid_ids)].reset_index(drop=True)\n",
        "\n",
        "    return train_dataframe, valid_dataframe\n",
        "\n",
        "# Function to get image embeddings\n",
        "\n",
        "def get_image_embeddings(valid_df, model_path):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    valid_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            valid_image_embeddings.append(image_embeddings)\n",
        "\n",
        "    return model, torch.cat(valid_image_embeddings)\n",
        "\n",
        "# Ensure dataset paths, configurations, and required functions are available and correct\n"
      ],
      "metadata": {
        "id": "RaXciTDLO5a5"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def build_loaders(dataframe, tokenizer, mode):\n",
        "    transforms = get_transforms(mode=mode)\n",
        "    dataset = CLIPDataset(\n",
        "        dataframe[\"image\"].values,\n",
        "        dataframe[\"caption\"].values,\n",
        "        tokenizer=tokenizer,\n",
        "        transforms=transforms,\n",
        "    )\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=CFG.batch_size,\n",
        "        num_workers=CFG.num_workers,\n",
        "        shuffle=True if mode == \"train\" else False,\n",
        "    )\n",
        "    return dataloader"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.041512,
          "end_time": "2021-04-05T08:01:52.054352",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.01284",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T09:00:02.739396Z",
          "iopub.execute_input": "2024-12-23T09:00:02.739691Z",
          "iopub.status.idle": "2024-12-23T09:00:02.747142Z",
          "shell.execute_reply.started": "2024-12-23T09:00:02.739668Z",
          "shell.execute_reply": "2024-12-23T09:00:02.746487Z"
        },
        "id": "h6BPSFTaD6sz"
      },
      "outputs": [],
      "execution_count": 75
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the next cell start training the model. Put the kernel on GPU mode. Every epoch should take about 24 minutes on GPU (even one epoch is enough!). It can take one minute before training actually starts because we are going to encode all the captions once in the train and valid dataset, so please don't stop it! Every thing is working fine."
      ],
      "metadata": {
        "id": "jgrAsGR8D6s1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "papermill": {
          "duration": 16449.265031,
          "end_time": "2021-04-05T12:36:01.33376",
          "exception": false,
          "start_time": "2021-04-05T08:01:52.068729",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-12-23T09:03:45.387981Z",
          "iopub.execute_input": "2024-12-23T09:03:45.388366Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "hLYlKf4uD6s7",
        "outputId": "b688b913-ef5b-41d9-f274-15d63ead469a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "CLIPDataset.__init__() got multiple values for argument 'tokenizer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-77-2c7f1fc384d8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_train_valid_dfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDistilBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-7da46fcdff7e>\u001b[0m in \u001b[0;36mbuild_loaders\u001b[0;34m(dataframe, tokenizer, mode)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_loaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     dataset = CLIPDataset(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"caption\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: CLIPDataset.__init__() got multiple values for argument 'tokenizer'"
          ]
        }
      ],
      "execution_count": 78
    },
    {
      "cell_type": "code",
      "source": [
        "torch.load('model_checkpoint.pth')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "JysiXTS3D6s8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "anwpPpcwD6s9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay! We are done with training the model. Now, we need to do inference which in our case will be giving the model a piece of text and want it to retrieve the most relevant images from an unseen validation (or test) set."
      ],
      "metadata": {
        "id": "B8jotrmuD6s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Image Embeddings"
      ],
      "metadata": {
        "id": "d8rsRxV1D6s-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself."
      ],
      "metadata": {
        "id": "9NZ2zpRCD6s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(valid_df, model_path):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    valid_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            valid_image_embeddings.append(image_embeddings)\n",
        "    return model, torch.cat(valid_image_embeddings)"
      ],
      "metadata": {
        "trusted": true,
        "id": "aopJHawnD6s_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "_, valid_df = make_train_valid_dfs()\n",
        "model, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "0dtR54FqD6tA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding Matches"
      ],
      "metadata": {
        "id": "53pPuS4tD6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function does the final task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. It will display the most relevant images from the validation set! Isn't it amazing? Let's see how it performs after all!"
      ],
      "metadata": {
        "id": "lBqfjjkwD6tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_matches(model, image_embeddings, query, image_filenames, n=9):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n",
        "    encoded_query = tokenizer([query])\n",
        "    batch = {\n",
        "        key: torch.tensor(values).to(CFG.device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "\n",
        "    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n",
        "    matches = [image_filenames[idx] for idx in indices[::5]]\n",
        "\n",
        "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    for match, ax in zip(matches, axes.flatten()):\n",
        "        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(image)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.025647,
          "end_time": "2021-04-05T12:36:01.385717",
          "exception": false,
          "start_time": "2021-04-05T12:36:01.36007",
          "status": "completed"
        },
        "tags": [],
        "trusted": true,
        "id": "Dz2HapHhD6tC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we use this function. Aaaannnndddd the results:"
      ],
      "metadata": {
        "id": "8uKYijD0D6tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_matches(model,\n",
        "             image_embeddings,\n",
        "             query=\"one dog sitting on the grass\",\n",
        "             image_filenames=valid_df['image'].values,\n",
        "             n=9)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RpFeEhEwD6tE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "KgE9XxlrD6tF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final words"
      ],
      "metadata": {
        "id": "RSiU_wM6D6tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I hope you have enjoyed this article. Implementing this paper was a really interesting experience for me. I want to thank Khalid Salama for the great Keras code example he provided which inspired me to write something similar in PyTorch."
      ],
      "metadata": {
        "id": "jllPZmbGD6tG"
      }
    }
  ]
}