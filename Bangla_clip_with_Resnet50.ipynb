{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQJY3d07U31q",
        "outputId": "aa75aea5-1d2e-4455-ced3-d072363ce8cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.15)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.24.0)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.9.2)\n",
            "Requirement already satisfied: albucore>=0.0.15 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.16)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers sentencepiece torch torchvision albumentations timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "evvQqujvamXV"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHz0psxzamY2",
        "outputId": "0fa71af4-c4e9-4940-c60e-5d733b1b8b67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Step 1: Mount Google Drive to access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "EclmxmhBambV"
      },
      "outputs": [],
      "source": [
        "# Define paths to the image folder and caption file\n",
        "image_folder = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/Flicker8k_Dataset'\n",
        "caption_file = '/content/drive/MyDrive/Bangla Image dataset with caption/Flickr8k_Dataset/BAN-Cap_captiondata.json'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "FXv9WTxtf0X1"
      },
      "outputs": [],
      "source": [
        "# Define configuration with adjusted batch size\n",
        "class CFG:\n",
        "    model_name = \"resnet50\"\n",
        "    text_encoder_model = \"sagorsarker/bangla-bert-base\"  # Bangla BERT model\n",
        "    pretrained = True\n",
        "    trainable = True\n",
        "    batch_size = 16  # Reduced batch size\n",
        "    size = 224\n",
        "    image_embedding = 2048\n",
        "    text_embedding = 768\n",
        "    projection_dim = 512\n",
        "    max_length = 128\n",
        "    temperature = 0.07\n",
        "    image_encoder_lr = 1e-4\n",
        "    text_encoder_lr = 1e-5\n",
        "    head_lr = 1e-3\n",
        "    weight_decay = 1e-4\n",
        "    patience = 2\n",
        "    factor = 0.5\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    epochs = 30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "xlHF5Tm_amg3"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, image_filenames, captions, tokenizer, transforms):\n",
        "        self.image_filenames = image_filenames\n",
        "        self.captions = list(captions)\n",
        "        self.encoded_captions = tokenizer(\n",
        "            list(captions), padding=True, truncation=True, max_length=CFG.max_length\n",
        "        )\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(values[idx])\n",
        "            for key, values in self.encoded_captions.items()\n",
        "        }\n",
        "        image = cv2.imread(f\"{image_folder}/{self.image_filenames[idx]}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = self.transforms(image=image)['image']\n",
        "        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n",
        "        item['caption'] = self.captions[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ivQTeP8Oamj1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Image Encoder\n",
        "class ImageEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "7iyNR7Gmammo"
      },
      "outputs": [],
      "source": [
        "# Text Encoder\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n",
        "        super().__init__()\n",
        "        self.model = AutoModel.from_pretrained(model_name)\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = trainable\n",
        "        self.target_token_idx = 0\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = output.last_hidden_state\n",
        "        return last_hidden_state[:, self.target_token_idx, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "oWR3sEhSampP"
      },
      "outputs": [],
      "source": [
        "# Projection Head\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, projection_dim=CFG.projection_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected = self.projection(x)\n",
        "        x = self.gelu(projected)\n",
        "        x = self.fc(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x + projected\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "d07Vai_7amt9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# CLIP Model\n",
        "class CLIPModel(nn.Module):\n",
        "    def __init__(self, temperature=CFG.temperature):\n",
        "        super().__init__()\n",
        "        self.image_encoder = ImageEncoder()\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.image_projection = ProjectionHead(embedding_dim=CFG.image_embedding)\n",
        "        self.text_projection = ProjectionHead(embedding_dim=CFG.text_embedding)\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def forward(self, batch):\n",
        "        image_features = self.image_encoder(batch[\"image\"])\n",
        "        text_features = self.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        image_embeddings = self.image_projection(image_features)\n",
        "        text_embeddings = self.text_projection(text_features)\n",
        "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
        "        images_similarity = image_embeddings @ image_embeddings.T\n",
        "        texts_similarity = text_embeddings @ text_embeddings.T\n",
        "        targets = F.softmax((images_similarity + texts_similarity) / 2 * self.temperature, dim=-1)\n",
        "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "        loss = (images_loss + texts_loss) / 2.0\n",
        "        return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7bcD0-znamwl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Cross Entropy\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    return loss.mean() if reduction == \"mean\" else loss\n",
        "\n",
        "# Data loaders\n",
        "def get_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(CFG.size, CFG.size, always_apply=True),\n",
        "        A.Normalize(max_pixel_value=255.0, always_apply=True),\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K8djbMmdJOx",
        "outputId": "37ba25fc-3a0e-4639-de4b-cc60fc540e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'caption_id': '1000268201_693b08cb0e.jpg#0', 'english_caption': 'A child in a pink dress is climbing up a set of stairs in an entry way .', 'bengali_caption': 'একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্রবেশ পথের সিঁড়ি বেয়ে উঠছে।'}\n"
          ]
        }
      ],
      "source": [
        "with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Print a sample item to inspect the structure\n",
        "print(captions_data[0])  # Check the first item for key names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "J7eEnEjqeq86"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F  # Add this line for functional operations\n",
        "import albumentations as A\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import timm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcJhOpB2gDnw",
        "outputId": "63f71799-f9ba-4a7d-8572-01a9775510f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-79-7371df4d553e>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()  # Mixed precision scaler\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2023 [00:00<?, ?it/s]<ipython-input-79-7371df4d553e>:9: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision training\n",
            "100%|██████████| 2023/2023 [08:24<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 23.2598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:23<00:00,  6.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.8378\n",
            "Best model saved!\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2023/2023 [08:02<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.1940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:25<00:00,  5.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.7849\n",
            "Best model saved!\n",
            "Epoch 3/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2023/2023 [08:00<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.9101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:23<00:00,  6.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.7858\n",
            "Epoch 4/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2023/2023 [08:04<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.8398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:23<00:00,  6.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.7855\n",
            "Epoch 5/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2023/2023 [08:02<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:24<00:00,  6.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.7564\n",
            "Best model saved!\n",
            "Epoch 6/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2023/2023 [07:59<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:22<00:00,  6.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.4900\n",
            "Best model saved!\n",
            "Epoch 7/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2023/2023 [08:02<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 506/506 [01:22<00:00,  6.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.6633\n",
            "Epoch 8/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/2023 [00:00<30:16,  1.11it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "# Optimized train_epoch with mixed precision\n",
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler()  # Mixed precision scaler\n",
        "    loss_meter = 0\n",
        "    for batch in tqdm(train_loader, total=len(train_loader)):\n",
        "        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision training\n",
        "            loss = model(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        loss_meter += loss.item()\n",
        "    return loss_meter / len(train_loader)\n",
        "\n",
        "# Validation function remains the same\n",
        "\n",
        "# Main function with optimizations for data loading and mixed precision training\n",
        "def main():\n",
        "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "        captions_data = json.load(f)\n",
        "\n",
        "    # Extract image files and captions based on provided structure\n",
        "    image_files = [item['caption_id'].split('#')[0] for item in captions_data]\n",
        "    captions = [item['bengali_caption'] for item in captions_data]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "    transforms = get_transforms()\n",
        "    dataset = CLIPDataset(image_files, captions, tokenizer, transforms)\n",
        "\n",
        "    # Data splitting into train and validation sets\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    valid_size = len(dataset) - train_size\n",
        "    train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "    # DataLoader with num_workers for faster data loading\n",
        "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=4)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "\n",
        "    params = [\n",
        "        {\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n",
        "        {\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n",
        "        {\"params\": itertools.chain(model.image_projection.parameters(), model.text_projection.parameters()),\n",
        "         \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n",
        "    ]\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params, weight_decay=0.0)\n",
        "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor)\n",
        "    step = \"epoch\"\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    for epoch in range(CFG.epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{CFG.epochs}\")\n",
        "\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "        valid_loss = valid_epoch(model, valid_loader)\n",
        "        print(f\"Validation Loss: {valid_loss:.4f}\")\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), \"best_clip_model_bangla.pt\")\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        if step == \"epoch\":\n",
        "            lr_scheduler.step(valid_loss)\n",
        "\n",
        "# Run the training with optimizations\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zmpc0cgirWoA"
      },
      "source": [
        "#Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rX8wC1PO7LhV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming CLIPDataset and CFG have already been defined\n",
        "# Ensure the paths to the dataset are properly set\n",
        "\n",
        "# Load and prepare the dataset\n",
        "with open(caption_file, 'r', encoding='utf-8') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Prepare image files and captions\n",
        "image_files = [item['caption_id'].split('#')[0] for item in captions_data]\n",
        "captions = [item['bengali_caption'] for item in captions_data]\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "transforms = get_transforms()\n",
        "\n",
        "# Create the dataset\n",
        "dataset = CLIPDataset(image_files, captions, tokenizer, transforms)\n",
        "\n",
        "# Split dataset into train and validation sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "valid_size = len(dataset) - train_size\n",
        "_, valid_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size])\n",
        "\n",
        "# Define the function to get image embeddings\n",
        "def get_image_embeddings(valid_dataset, model_path):\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Load the trained model\n",
        "    model = CLIPModel().to(CFG.device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n",
        "    model.eval()\n",
        "\n",
        "    valid_image_embeddings = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(valid_loader):\n",
        "            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n",
        "            image_embeddings = model.image_projection(image_features)\n",
        "            valid_image_embeddings.append(image_embeddings)\n",
        "\n",
        "    return model, torch.cat(valid_image_embeddings)\n",
        "\n",
        "# Perform inference to get image embeddings from the validation set\n",
        "model, image_embeddings = get_image_embeddings(valid_dataset, \"/content/best_clip_model_bangla.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYdu6enJrZSV"
      },
      "outputs": [],
      "source": [
        "def find_matches(model, image_embeddings, query, image_files, n=9):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CFG.text_encoder_model)\n",
        "\n",
        "    # Encode the query and print to ensure uniqueness\n",
        "    encoded_query = tokenizer([query], return_tensors=\"pt\", padding=True, truncation=True, max_length=CFG.max_length)\n",
        "    print(f\"Encoded Query: {encoded_query}\")  # Check if different prompts yield different encodings\n",
        "\n",
        "    batch = {\n",
        "        key: torch.tensor(values).to(CFG.device)\n",
        "        for key, values in encoded_query.items()\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        text_features = model.text_encoder(\n",
        "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
        "        )\n",
        "        text_embeddings = model.text_projection(text_features)\n",
        "\n",
        "    # Normalize embeddings and print for debugging\n",
        "    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n",
        "    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n",
        "\n",
        "    print(f\"Text Embeddings: {text_embeddings_n}\")  # Check if text embeddings change with different prompts\n",
        "\n",
        "    # Calculate similarity and retrieve top matches\n",
        "    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n",
        "    values, indices = torch.topk(dot_similarity.squeeze(0), n)\n",
        "\n",
        "    matches = [image_files[idx] for idx in indices]\n",
        "    print(f\"Top match values: {values}\")  # To see if similarity scores vary\n",
        "\n",
        "    # Display the matched images\n",
        "    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "    for match, ax in zip(matches, axes.flatten()):\n",
        "        image = cv2.imread(f\"{image_folder}/{match}\")\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        ax.imshow(image)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiPDBptxrZQG"
      },
      "outputs": [],
      "source": [
        "prompt = \"কুকুর খেলা করছে\"  # Example prompt for \"A dog is playing\"\n",
        "find_matches(model, image_embeddings, prompt, image_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Path to your saved model file\n",
        "model_path = \"best_clip_model_bangla.pt\"  # Replace with your actual model file name if different\n",
        "\n",
        "# Download the file\n",
        "files.download(model_path)\n"
      ],
      "metadata": {
        "id": "mHNFLmICA1kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T390_6NgA2vu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}